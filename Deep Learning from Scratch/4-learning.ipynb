{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "무식한 신경망 학습\n",
    "========\n",
    "MNIST 이미지 인식 신경망을 역전파 없이 만들어보자. 아래와 같은 구조로 만들것임.\n",
    "\n",
    "> 입력층(784) &rarr; 은닉층(50 or 100) &rarr; 시그모이드 &rarr; 결과(10) &rarr; 소프트맥스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 시작!\n",
      "\n",
      "반복횟수\t정확도\tLoss\n",
      "-------------------------------------------\n",
      "       0\t8.0%\t2.3048960747448994\n",
      "\n",
      "\u001b[31mInterrupted!!!\u001b[31m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import signal\n",
    "import sys\n",
    "import mnist\n",
    "\n",
    "#\n",
    "# Hyper parameters\n",
    "#\n",
    "# 히든레이어 뉴런 수 (ex: 50, 100)\n",
    "HIDDEN_LAYER_SIZE = 50\n",
    "# 정규분포 난수로 생성될 초기 가중치의 표준편차\n",
    "WEIGHT_INIT_STD = 0.01\n",
    "# 경사하강법을 몇번 적용할지\n",
    "ITERATION_COUNT = 3\n",
    "# 학습에 사용할 미니배치의 크기\n",
    "BATCH_SIZE = 100\n",
    "# 학습률\n",
    "LEARNING_RATE = 10\n",
    "# 에퍼크, 학습 진척도를 얼마나 자주 표시할지 (ex: 100, 300)\n",
    "EPOCH = 1\n",
    "\n",
    "#\n",
    "# Utility functions\n",
    "#\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "def gradient_sigmoid(x):\n",
    "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
    "\n",
    "def softmax(A):\n",
    "    extend = (lambda x:x) if A.ndim == 1 else (lambda x: x[..., np.newaxis])\n",
    "    ExpA = np.exp(A - extend(A.max(axis=-1)))\n",
    "    return ExpA / extend(ExpA.sum(axis=-1))\n",
    "\n",
    "def cross_entropy_error(expected, actual):\n",
    "    epsilon = 1E-7\n",
    "    return -(actual * np.log(expected + epsilon)).sum(axis=-1)\n",
    "def cross_entropy_error_batch(*args):\n",
    "    return cross_entropy_error(*args).mean()\n",
    "\n",
    "def predict(input):\n",
    "    def network(w0, b0, w1, b1):\n",
    "        a0 = input @ w0 + b0\n",
    "        z0 = sigmoid(a0)\n",
    "        a1 = z0 @ w1 + b1\n",
    "        z1 = softmax(a1)\n",
    "        return [a0, z0, a1, z1]\n",
    "    return network\n",
    "\n",
    "def accuracy(expected, actual):\n",
    "    return (expected.argmax(axis=-1) == actual.argmax(axis=-1)).mean()\n",
    "\n",
    "#\n",
    "# Main logic\n",
    "#\n",
    "MNIST = mnist.load()\n",
    "TRAIN_IMG = MNIST['train_img']\n",
    "TRAIN_LABEL = MNIST['train_label']\n",
    "\n",
    "layer0_size = TRAIN_IMG.shape[-1]\n",
    "layer1_size = HIDDEN_LAYER_SIZE\n",
    "layer2_size = TRAIN_LABEL.shape[-1]\n",
    "\n",
    "# Randomly initialize the parameters\n",
    "parameters = [\n",
    "    # w0\n",
    "    WEIGHT_INIT_STD * np.random.randn(layer0_size, layer1_size), \n",
    "    # b0\n",
    "    np.zeros(layer1_size),\n",
    "    # w1\n",
    "    WEIGHT_INIT_STD * np.random.randn(layer1_size, layer2_size),\n",
    "    # b1\n",
    "    np.zeros(layer2_size),\n",
    "]\n",
    "\n",
    "print('''학습 시작!\n",
    "\n",
    "반복횟수\\t정확도\\tLoss\n",
    "-------------------------------------------''')\n",
    "\n",
    "try:\n",
    "    for iteration in range(ITERATION_COUNT):\n",
    "        # Sample a batch from the train image/label set\n",
    "        sample = np.random.choice(TRAIN_IMG.shape[0], BATCH_SIZE)\n",
    "        BATCH_IMG = TRAIN_IMG[sample]\n",
    "        BATCH_LABEL = TRAIN_LABEL[sample]\n",
    "\n",
    "        network = predict(BATCH_IMG)\n",
    "\n",
    "        # Try the result\n",
    "        if iteration % EPOCH == 0:\n",
    "            expected = network(*parameters)[-1]\n",
    "            percentage = accuracy(expected, BATCH_LABEL)*100\n",
    "            loss = cross_entropy_error_batch(expected, BATCH_LABEL)\n",
    "            print(f'{iteration:8}\\t{percentage:.04}%\\t{loss}')\n",
    "\n",
    "        # Calculate gradient\n",
    "        def grad(parameters, h=1E-4):\n",
    "            def loss_function(*arguments):\n",
    "                expected = network(*arguments)[-1]\n",
    "                return cross_entropy_error_batch(expected, BATCH_LABEL)\n",
    "            def grad(param):\n",
    "                shape = param.shape\n",
    "                gradient = np.empty(shape)\n",
    "                for j in np.ndindex(shape):\n",
    "                    orig = param[j]\n",
    "                    param[j] = orig + h\n",
    "                    y2 = loss_function(*parameters)\n",
    "                    param[j] = orig - h\n",
    "                    y1 = loss_function(*parameters)\n",
    "                    param[j] = orig\n",
    "                    gradient[j] = (y2 - y1)/(2*h)\n",
    "                return gradient\n",
    "            return [grad(param) for param in parameters]\n",
    "\n",
    "        # Update parameters using gradient descent method\n",
    "        gradient = grad(parameters)\n",
    "        for param, grad in zip(parameters, gradient):\n",
    "            param -= LEARNING_RATE * grad\n",
    "\n",
    "    expected = predict(MNIST['test_img'])(*parameters)[-1]\n",
    "    TEST_LABEL = MNIST['test_label']\n",
    "    percentage = accuracy(expected, TEST_LABEL)*100\n",
    "\n",
    "    print(f'''\n",
    "    학습 완료!\n",
    "\n",
    "    최종 점수\n",
    "    -------------\n",
    "    정확도 : {percentage}%\n",
    "    ''')\n",
    "except KeyboardInterrupt:\n",
    "    print('\\n\\x1b[31mInterrupted!!!\\x1b[31m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**느려서 못해먹겠다!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
