{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "무식한 신경망 학습\n",
    "========\n",
    "MNIST 이미지 인식 신경망을 역전파 없이 만들어보자. 아래와 같은 구조로 만들것임.\n",
    "\n",
    "> 입력층(784) &rarr; 은닉층(50 or 100) &rarr; 시그모이드 &rarr; 결과(10) &rarr; 소프트맥스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 시작!\n",
      "\n",
      "반복횟수\t정확도\tLoss\n",
      "-------------------------------------------\n",
      "       0\t11.0%\t2.3057093326098355\n",
      "     300\t56.0%\t1.6630170021656108\n",
      "     600\t79.0%\t0.8275390509215265\n",
      "     900\t85.0%\t0.5581177557098227\n",
      "    1200\t92.0%\t0.40288390492765574\n",
      "    1500\t88.0%\t0.49844538135890226\n",
      "    1800\t89.0%\t0.31785818016998824\n",
      "    2100\t90.0%\t0.28806267233478844\n",
      "    2400\t88.0%\t0.35375411964537357\n",
      "    2700\t92.0%\t0.3196433350073984\n",
      "    3000\t90.0%\t0.287587011873968\n",
      "    3300\t95.0%\t0.2596750026306412\n",
      "    3600\t90.0%\t0.34085480786688627\n",
      "    3900\t91.0%\t0.2730142301537956\n",
      "    4200\t92.0%\t0.2134652409435964\n",
      "    4500\t92.0%\t0.28057109553390236\n",
      "    4800\t95.0%\t0.3384401103156803\n",
      "    5100\t90.0%\t0.3162968517330497\n",
      "    5400\t95.0%\t0.1676589282608701\n",
      "    5700\t94.0%\t0.17557476433573616\n",
      "    6000\t91.0%\t0.34179091873328415\n",
      "    6300\t94.0%\t0.2420873021031243\n",
      "    6600\t92.0%\t0.273471817437171\n",
      "    6900\t96.0%\t0.15991521303709477\n",
      "    7200\t90.0%\t0.31844268157773137\n",
      "    7500\t95.0%\t0.15652490922762363\n",
      "    7800\t94.0%\t0.22284897309411797\n",
      "    8100\t94.0%\t0.16011577804418964\n",
      "    8400\t96.0%\t0.14079020885702936\n",
      "    8700\t95.0%\t0.21822769701348346\n",
      "    9000\t97.0%\t0.10424705171759632\n",
      "    9300\t97.0%\t0.15594892409781655\n",
      "    9600\t94.0%\t0.20622770199399837\n",
      "    9900\t94.0%\t0.1615662586133711\n",
      "\n",
      "학습 완료!\n",
      "\n",
      "최종 점수\n",
      "-------------\n",
      "정확도 : 94.66%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mnist\n",
    "import numpy as np\n",
    "\n",
    "#\n",
    "# Hyper parameters\n",
    "#\n",
    "# 히든레이어 뉴런 수 (ex: 50, 100)\n",
    "HIDDEN_LAYER_SIZE = 50\n",
    "# 정규분포 난수로 생성될 초기 가중치의 표준편차\n",
    "WEIGHT_INIT_STD = 0.01\n",
    "# 경사하강법을 몇번 적용할지\n",
    "ITERATION_COUNT = 10000\n",
    "# 학습에 사용할 미니배치의 크기\n",
    "BATCH_SIZE = 100\n",
    "# 학습률\n",
    "LEARNING_RATE = 0.1\n",
    "# 에퍼크, 학습 진척도를 얼마나 자주 표시할지 (ex: 100, 300)\n",
    "EPOCH = 300\n",
    "\n",
    "#\n",
    "# Utility functions\n",
    "#\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "def gradient_sigmoid(x):\n",
    "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
    "\n",
    "def softmax(A):\n",
    "    extend = (lambda x:x) if A.ndim == 1 else (lambda x: x[..., np.newaxis])\n",
    "    ExpA = np.exp(A - extend(A.max(axis=-1)))\n",
    "    return ExpA / extend(ExpA.sum(axis=-1))\n",
    "\n",
    "def cross_entropy_error(expected, actual):\n",
    "    epsilon = 1E-7\n",
    "    return -(actual * np.log(expected + epsilon)).sum(axis=-1)\n",
    "def cross_entropy_error_batch(*args):\n",
    "    return cross_entropy_error(*args).mean()\n",
    "\n",
    "def predict(input):\n",
    "    def network(w0, b0, w1, b1):\n",
    "        a0 = input @ w0 + b0\n",
    "        z0 = sigmoid(a0)\n",
    "        a1 = z0 @ w1 + b1\n",
    "        z1 = softmax(a1)\n",
    "        return [a0, z0, a1, z1]\n",
    "    return network\n",
    "\n",
    "def accuracy(expected, actual):\n",
    "    return (expected.argmax(axis=-1) == actual.argmax(axis=-1)).mean()\n",
    "\n",
    "#\n",
    "# Main logic\n",
    "#\n",
    "MNIST = mnist.load()\n",
    "TRAIN_IMG = MNIST['train_img']\n",
    "TRAIN_LABEL = MNIST['train_label']\n",
    "\n",
    "layer0_size = TRAIN_IMG.shape[-1]\n",
    "layer1_size = HIDDEN_LAYER_SIZE\n",
    "layer2_size = TRAIN_LABEL.shape[-1]\n",
    "\n",
    "# Randomly initialize the parameters\n",
    "parameters = [\n",
    "    # w0\n",
    "    WEIGHT_INIT_STD * np.random.randn(layer0_size, layer1_size), \n",
    "    # b0\n",
    "    np.zeros(layer1_size),\n",
    "    # w1\n",
    "    WEIGHT_INIT_STD * np.random.randn(layer1_size, layer2_size),\n",
    "    # b1\n",
    "    np.zeros(layer2_size),\n",
    "]\n",
    "\n",
    "print('''학습 시작!\n",
    "\n",
    "반복횟수\\t정확도\\tLoss\n",
    "-------------------------------------------''')\n",
    "for iteration in range(ITERATION_COUNT):\n",
    "    # Sample a batch from the train image/label set\n",
    "    sample = np.random.choice(TRAIN_IMG.shape[0], BATCH_SIZE)\n",
    "    BATCH_IMG = TRAIN_IMG[sample]\n",
    "    BATCH_LABEL = TRAIN_LABEL[sample]\n",
    "\n",
    "    network = predict(BATCH_IMG)\n",
    "\n",
    "    # Try the result\n",
    "    if iteration % EPOCH == 0:\n",
    "        expected = network(*parameters)[-1]\n",
    "        percentage = accuracy(expected, BATCH_LABEL)*100\n",
    "        loss = cross_entropy_error_batch(expected, BATCH_LABEL)\n",
    "        print(f'{iteration:8}\\t{percentage:.04}%\\t{loss}')\n",
    "\n",
    "    # Calculate gradient\n",
    "    loss_function = lambda *arguments: cross_entropy_error_batch(network(*arguments)[-1], BATCH_LABEL)\n",
    "    def grad_naive(parameters, h=1E-4):\n",
    "        def grad(param):\n",
    "            shape = param.shape\n",
    "            gradient = np.empty(shape)\n",
    "            for j in np.ndindex(shape):\n",
    "                orig = param[j]\n",
    "                param[j] = orig + h\n",
    "                y2 = loss_function(*parameters)\n",
    "                param[j] = orig - h\n",
    "                y1 = loss_function(*parameters)\n",
    "                param[j] = orig\n",
    "                gradient[j] = (y2 - y1)/(2*h)\n",
    "            return gradient\n",
    "        #return [np.zeros(parameters[0].shape), *(grad(param) for param in parameters[1:])]\n",
    "        return [grad(param) for param in parameters]\n",
    "    def grad_analytic(parameters):\n",
    "        w1 = parameters[2]\n",
    "        a0, z0, _, expected = network(*parameters)\n",
    "        \n",
    "        # Backward propagation\n",
    "        dz1 = (expected - BATCH_LABEL)/BATCH_SIZE\n",
    "        dw1 = z0.T @ dz1\n",
    "        db1 = dz1.sum(axis=0)\n",
    "        \n",
    "        dz0 = gradient_sigmoid(a0) * (dz1 @ w1.T)\n",
    "        dw0 = BATCH_IMG.T @ dz0\n",
    "        db0 = dz0.sum(axis=0)\n",
    "        return [dw0, db0, dw1, db1]\n",
    "\n",
    "    # Update parameters using gradient descent method\n",
    "    gradient = grad_analytic(parameters)\n",
    "    for param, grad in zip(parameters, gradient):\n",
    "        param -= LEARNING_RATE * grad\n",
    "\n",
    "expected = predict(MNIST['test_img'])(*parameters)[-1]\n",
    "TEST_LABEL = MNIST['test_label']\n",
    "percentage = accuracy(expected, TEST_LABEL)*100\n",
    "\n",
    "print(f'''\n",
    "학습 완료!\n",
    "\n",
    "최종 점수\n",
    "-------------\n",
    "정확도 : {percentage}%\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOs\n",
    "1. 수치미분으로 구한것과, 해석적으로 구한 두 그래디언트의 값이 실제로 얼마나 비슷한지 체크\n",
    "2. numpy-mkl로 라이브러리 교체하고 성능향상 보기"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
