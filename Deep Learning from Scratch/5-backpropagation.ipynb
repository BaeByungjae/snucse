{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "오차역전파법 <small>Backpropagation</small>\n",
    "========\n",
    "그래디언트를 컴퓨터로 Numerical하게 계산하려니 너무 시간이 오래걸린다. 미분을 열심히 해서, 해석적으로 구해보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m1. 계산시간 비교\u001b[0m\n",
      "해석적으로 계산하는데에 든 시간 : 0.01045초\n",
      "수치적으로 계산하는데에 든 시간 : 104.7초\n",
      "\n",
      "\n",
      "\u001b[31m2. 그래디언트값 생김새\u001b[0m\n",
      "  ∇W0\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      "  ∇B0\n",
      "[-0.00024 -0.00014 -0.00009 ..., -0.0002   0.0002   0.00002]\n",
      "\n",
      "  ∇W1\n",
      "[[-0.00421 -0.00666 -0.00503 ..., -0.00979 -0.00292  0.00547]\n",
      " [-0.00452 -0.00599 -0.00474 ..., -0.00957 -0.00249  0.00504]\n",
      " [-0.00528 -0.00609 -0.00514 ..., -0.00973 -0.00288  0.00551]\n",
      " ..., \n",
      " [-0.00399 -0.00527 -0.00523 ..., -0.00982 -0.00208  0.00652]\n",
      " [-0.00378 -0.00552 -0.00496 ..., -0.01071 -0.00194  0.00526]\n",
      " [-0.0042  -0.00608 -0.00514 ..., -0.00989 -0.0023   0.00589]]\n",
      "\n",
      "  ∇B1\n",
      "[-0.00896 -0.01163 -0.01058 ..., -0.01955 -0.00473  0.01166]\n",
      "\n",
      "\n",
      "\u001b[31m3. 해석적으로 구한 그래디언트와, 수치적으로 구한 그래디언트가 얼마나 비슷할까?\u001b[0m\n",
      "항목\t행렬 원소 이차평균\t수치적으로 계산한것과의 오차\n",
      "------------------------------------------------------------\n",
      "∇W0\t           0.00011\t                     0.00011\n",
      "∇B0\t           0.00021\t                     2.1e-10\n",
      "∇W1\t             0.013\t                     1.3e-08\n",
      "∇B1\t             0.026\t                     2.7e-08\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import mnist\n",
    "\n",
    "#\n",
    "# Hyper parameters\n",
    "#\n",
    "# 히든레이어 뉴런 수 (ex: 50, 100)\n",
    "HIDDEN_LAYER_SIZE = 50\n",
    "# 정규분포 난수로 생성될 초기 가중치의 표준편차\n",
    "WEIGHT_INIT_STD = 0.01\n",
    "# 학습에 사용할 미니배치의 크기\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "#\n",
    "# Utility functions\n",
    "#\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "def gradient_sigmoid(x):\n",
    "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
    "\n",
    "def softmax(A):\n",
    "    extend = (lambda x:x) if A.ndim == 1 else (lambda x: x[..., np.newaxis])\n",
    "    ExpA = np.exp(A - extend(A.max(axis=-1)))\n",
    "    return ExpA / extend(ExpA.sum(axis=-1))\n",
    "\n",
    "def cross_entropy_error(expected, actual):\n",
    "    epsilon = 1E-7\n",
    "    return -(actual * np.log(expected + epsilon)).sum(axis=-1)\n",
    "def cross_entropy_error_batch(*args):\n",
    "    return cross_entropy_error(*args).mean()\n",
    "\n",
    "#\n",
    "# Main logic\n",
    "#\n",
    "MNIST = mnist.load()\n",
    "TRAIN_IMG = MNIST['train_img']\n",
    "TRAIN_LABEL = MNIST['train_label']\n",
    "\n",
    "# 각 레이어의 뉴런 수\n",
    "layer0_size = TRAIN_IMG.shape[-1]\n",
    "layer1_size = HIDDEN_LAYER_SIZE\n",
    "layer2_size = TRAIN_LABEL.shape[-1]\n",
    "\n",
    "# 랜덤으로 웨이트 행렬과 가중치 벡터 초기화\n",
    "parameters = [\n",
    "    # w0\n",
    "    WEIGHT_INIT_STD * np.random.randn(layer0_size, layer1_size), \n",
    "    # b0\n",
    "    np.zeros(layer1_size),\n",
    "    # w1\n",
    "    WEIGHT_INIT_STD * np.random.randn(layer1_size, layer2_size),\n",
    "    # b1\n",
    "    np.zeros(layer2_size),\n",
    "]\n",
    "\n",
    "# 미니배치: 트레인셋에서 그래디언트 계산용으로 쓸 데이터 N개 임의추출\n",
    "indices = np.random.choice(TRAIN_IMG.shape[0], BATCH_SIZE)\n",
    "BATCH_IMG, BATCH_LABEL = TRAIN_IMG[indices], TRAIN_LABEL[indices]\n",
    "\n",
    "def predict(w0, b0, w1, b1):\n",
    "    a0 = input @ w0 + b0\n",
    "    z0 = sigmoid(a0)\n",
    "    a1 = z0 @ w1 + b1\n",
    "    z1 = softmax(a1)\n",
    "    return [a0, z0, a1, z1]\n",
    "\n",
    "def grad_analytic(parameters):\n",
    "    w1 = parameters[2]\n",
    "    a0, z0, _, expected = network(*parameters)\n",
    "\n",
    "    # Backward propagation\n",
    "    dz1 = (expected - BATCH_LABEL)/BATCH_SIZE\n",
    "    dw1 = z0.T @ dz1\n",
    "    db1 = dz1.sum(axis=0)\n",
    "\n",
    "    dz0 = gradient_sigmoid(a0) * (dz1 @ w1.T)\n",
    "    dw0 = BATCH_IMG.T @ dz0\n",
    "    db0 = dz0.sum(axis=0)\n",
    "    return [dw0, db0, dw1, db1]\n",
    "\n",
    "def grad_numerical(parameters, h=1E-4):\n",
    "    def loss_function(*arguments):\n",
    "        expected = network(*arguments)[-1]\n",
    "        return cross_entropy_error_batch(expected, BATCH_LABEL)\n",
    "    def grad(param):\n",
    "        shape = param.shape\n",
    "        gradient = np.empty(shape)\n",
    "        for j in np.ndindex(shape):\n",
    "            orig = param[j]\n",
    "            param[j] = orig + h\n",
    "            y2 = loss_function(*parameters)\n",
    "            param[j] = orig - h\n",
    "            y1 = loss_function(*parameters)\n",
    "            param[j] = orig\n",
    "            gradient[j] = (y2 - y1)/(2*h)\n",
    "        return gradient\n",
    "    return [grad(param) for param in parameters]\n",
    "\n",
    "print('\\x1b[31m1. 계산시간 비교\\x1b[0m')\n",
    "# 해석적으로 그래디언트 계산\n",
    "t0 = time.time()\n",
    "analytic = grad_analytic(parameters)\n",
    "t1 = time.time()\n",
    "print(f'해석적으로 계산하는데에 든 시간 : {t1 - t0:.04}초')\n",
    "# 수치적으로 그래디언트 계산\n",
    "t0 = time.time()\n",
    "numerical = grad_numerical(parameters)\n",
    "t1 = time.time()\n",
    "print(f'수치적으로 계산하는데에 든 시간 : {t1 - t0:.04}초\\n\\n')\n",
    "\n",
    "# 행렬 출력\n",
    "names = ['∇W0', '∇B0', '∇W1', '∇B1']\n",
    "np.set_printoptions(suppress=True, precision=5, threshold=5)\n",
    "print('\\x1b[31m2. 그래디언트값 생김새\\x1b[0m')\n",
    "for name, mat in zip(names, analytic): print(f'  {name}\\n{mat}\\n')\n",
    "\n",
    "# 해석적으로 계산한것과, 수치적으로 계산한것 비교\n",
    "print('''\n",
    "\\x1b[31m3. 해석적으로 구한 그래디언트와, 수치적으로 구한 그래디언트가 얼마나 비슷할까?\\x1b[0m\n",
    "항목\\t행렬 원소 이차평균\\t수치적으로 계산한것과의 오차\n",
    "------------------------------------------------------------''')\n",
    "for analytic, numerical, name in zip(analytic, numerical, names):\n",
    "    mean = np.sqrt((analytic**2).mean())\n",
    "    error = np.sqrt(((analytic - numerical)**2).mean())\n",
    "\n",
    "    print(f'{name}\\t{mean:18.02}\\t{error:28.02}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
