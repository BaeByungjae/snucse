{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "오차역전파법 <small>Backpropagation</small>\n",
    "========\n",
    "그래디언트를 컴퓨터로 Numerical하게 계산하려니 너무 시간이 오래걸린다. 미분을 열심히 해서, 해석적으로 구해보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m1. 계산시간 비교\u001b[0m\n",
      "해석적으로 계산하는데에 든 시간 : 0.01059초\n",
      "수치적으로 계산하는데에 든 시간 : 114.0초\n",
      "\n",
      "\n",
      "\u001b[31m2. 그래디언트값 생김새\u001b[0m\n",
      "  ∇W0\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      "  ∇B0\n",
      "[-0.00001  0.00018 -0.00063 ...,  0.00001  0.00008  0.00025]\n",
      "\n",
      "  ∇W1\n",
      "[[-0.01316 -0.01855  0.01302 ...,  0.00942  0.01075 -0.02383]\n",
      " [-0.01078 -0.02     0.0136  ...,  0.01091  0.00979 -0.02437]\n",
      " [-0.01142 -0.02224  0.01473 ...,  0.01124  0.01004 -0.02444]\n",
      " ..., \n",
      " [-0.00793 -0.02075  0.01371 ...,  0.00996  0.00993 -0.02591]\n",
      " [-0.00952 -0.02149  0.01331 ...,  0.01265  0.0091  -0.02555]\n",
      " [-0.01189 -0.02007  0.0152  ...,  0.01037  0.00919 -0.0253 ]]\n",
      "\n",
      "  ∇B1\n",
      "[-0.0237  -0.03996  0.02764 ...,  0.02184  0.01955 -0.04861]\n",
      "\n",
      "\n",
      "\u001b[31m3. 해석적으로 구한 그래디언트와, 수치적으로 구한 그래디언트가 얼마나 비슷할까?\u001b[0m\n",
      "항목\t행렬 원소 이차평균\t수치적으로 계산한것과의 오차\n",
      "------------------------------------------------------------\n",
      "∇W0\t           0.00013\t                     1.3e-10\n",
      "∇B0\t           0.00022\t                     2.2e-10\n",
      "∇W1\t             0.014\t                     1.4e-08\n",
      "∇B1\t             0.027\t                     2.7e-08\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import mnist\n",
    "from common import sigmoid, gradient_sigmoid, softmax, cross_entropy_error_batch\n",
    "\n",
    "#\n",
    "# Hyper parameters\n",
    "#\n",
    "# 히든레이어 뉴런 수 (ex: 50, 100)\n",
    "HIDDEN_LAYER_SIZE = 50\n",
    "# 정규분포 난수로 생성될 초기 가중치의 표준편차\n",
    "WEIGHT_INIT_STD = 0.01\n",
    "# 학습에 사용할 미니배치의 크기\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "#\n",
    "# Main logic\n",
    "#\n",
    "MNIST = mnist.load()\n",
    "TRAIN_IMG = MNIST['train_img']\n",
    "TRAIN_LABEL = MNIST['train_label']\n",
    "\n",
    "# 각 레이어의 뉴런 수\n",
    "layer0_size = TRAIN_IMG.shape[-1]\n",
    "layer1_size = HIDDEN_LAYER_SIZE\n",
    "layer2_size = TRAIN_LABEL.shape[-1]\n",
    "\n",
    "# 랜덤으로 웨이트 행렬과 가중치 벡터 초기화\n",
    "parameters = [\n",
    "    # w0\n",
    "    WEIGHT_INIT_STD * np.random.randn(layer0_size, layer1_size), \n",
    "    # b0\n",
    "    np.zeros(layer1_size),\n",
    "    # w1\n",
    "    WEIGHT_INIT_STD * np.random.randn(layer1_size, layer2_size),\n",
    "    # b1\n",
    "    np.zeros(layer2_size),\n",
    "]\n",
    "\n",
    "# 미니배치: 트레인셋에서 그래디언트 계산용으로 쓸 데이터 N개 임의추출\n",
    "indices = np.random.choice(TRAIN_IMG.shape[0], BATCH_SIZE)\n",
    "BATCH_IMG, BATCH_LABEL = TRAIN_IMG[indices], TRAIN_LABEL[indices]\n",
    "\n",
    "def predict(w0, b0, w1, b1):\n",
    "    a0 = BATCH_IMG @ w0 + b0\n",
    "    z0 = sigmoid(a0)\n",
    "    a1 = z0 @ w1 + b1\n",
    "    z1 = softmax(a1)\n",
    "    return [a0, z0, a1, z1]\n",
    "\n",
    "def grad_analytic(parameters):\n",
    "    w1 = parameters[2]\n",
    "    a0, z0, _, expected = predict(*parameters)\n",
    "\n",
    "    # Backward propagation\n",
    "    dz1 = (expected - BATCH_LABEL)/BATCH_SIZE\n",
    "    dw1 = z0.T @ dz1\n",
    "    db1 = dz1.sum(axis=0)\n",
    "\n",
    "    dz0 = gradient_sigmoid(a0) * (dz1 @ w1.T)\n",
    "    dw0 = BATCH_IMG.T @ dz0\n",
    "    db0 = dz0.sum(axis=0)\n",
    "    return [dw0, db0, dw1, db1]\n",
    "\n",
    "def grad_numerical(parameters, h=1E-4):\n",
    "    def loss_function(*arguments):\n",
    "        expected = predict(*arguments)[-1]\n",
    "        return cross_entropy_error_batch(expected, BATCH_LABEL)\n",
    "    def grad(param):\n",
    "        shape = param.shape\n",
    "        gradient = np.empty(shape)\n",
    "        for j in np.ndindex(shape):\n",
    "            orig = param[j]\n",
    "            param[j] = orig + h\n",
    "            y2 = loss_function(*parameters)\n",
    "            param[j] = orig - h\n",
    "            y1 = loss_function(*parameters)\n",
    "            param[j] = orig\n",
    "            gradient[j] = (y2 - y1)/(2*h)\n",
    "        return gradient\n",
    "    return [grad(param) for param in parameters]\n",
    "\n",
    "print('\\x1b[31m1. 계산시간 비교\\x1b[0m')\n",
    "# 해석적으로 그래디언트 계산\n",
    "t0 = time.time()\n",
    "analytic = grad_analytic(parameters)\n",
    "t1 = time.time()\n",
    "print(f'해석적으로 계산하는데에 든 시간 : {t1 - t0:.04}초')\n",
    "# 수치적으로 그래디언트 계산\n",
    "t0 = time.time()\n",
    "numerical = grad_numerical(parameters)\n",
    "t1 = time.time()\n",
    "print(f'수치적으로 계산하는데에 든 시간 : {t1 - t0:.04}초\\n\\n')\n",
    "\n",
    "# 행렬 출력\n",
    "names = ['∇W0', '∇B0', '∇W1', '∇B1']\n",
    "np.set_printoptions(suppress=True, precision=5, threshold=5)\n",
    "print('\\x1b[31m2. 그래디언트값 생김새\\x1b[0m')\n",
    "for name, mat in zip(names, analytic): print(f'  {name}\\n{mat}\\n')\n",
    "\n",
    "# 해석적으로 계산한것과, 수치적으로 계산한것 비교\n",
    "print('''\n",
    "\\x1b[31m3. 해석적으로 구한 그래디언트와, 수치적으로 구한 그래디언트가 얼마나 비슷할까?\\x1b[0m\n",
    "항목\\t행렬 원소 이차평균\\t수치적으로 계산한것과의 오차\n",
    "------------------------------------------------------------''')\n",
    "for analytic, numerical, name in zip(analytic, numerical, names):\n",
    "    mean = np.sqrt((analytic**2).mean())\n",
    "    error = np.sqrt(((analytic - numerical)**2).mean())\n",
    "\n",
    "    print(f'{name}\\t{mean:18.02}\\t{error:28.02}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
