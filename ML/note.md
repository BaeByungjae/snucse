Introduction to Machine Learning
========

> 9월 1일

안녕하세요.

강의실이 모자라네? 드랍할사람 많을테니까.. 빨리 드랍하세요.

오늘 해보고, 다음시간까지 해보고 강의실 진짜 필요하면 옮길게요.

요즘 머신러닝이 안쓰이는데가 없어서, 요번학기도 전공이 다양하네. 컴공이 많긴하고
기계항공, 전기전보, 과학생물, 산업공학 다있어요. 자전도있어. 여기 뇌와 계산으로
신청한사람? 다른사람은요? 없어요? 원래 연합전공에 뇌 마음 프로그래밍 이런게
있는데 합반하기로했어요.

머신러닝이, 사실은 AI가. 인공지능이 역사가 오래돼요. 머신러닝을 바라보는 관점이
여럿이 있어요. 통계학, 계산이론, 등등. 근데 아직은 인공지능으로 바라보는 시각이
강한것같아요. 머신러닝이 응용폭이 굉장히 커요. 반도체공정에서 생기는 데이터도
머신러닝으로 분석하고. 삼성전자에서도 강의를 여러번 했어요. 다들 알다시피
인터넷이나 쇼핑몰같은곳에서도 비즈니스에 직접 쓸 수 있고, 구글 페이스북 갈라면
머신러닝을 해야될거에요.

공고가 된 실라부스가 있는데, 내가 강의자료를 다시 정리하려고 생각하고있어서,
전체적인 내용은 비슷할텐데 순서가 좀 달라질것같아요. 오늘 할 일은, 이 과목을
계속 들을거냐 에 도움을 주면 될것같아요. 어떤 내용을 배울지 개괄적으로 한시간만
수업할게요.

평가방식 이런거 먼저 얘기를 할까?

* 오픈북 시험 두번 (50%)
* 미니 프로젝트 두번, 포스터 1회 (40%)

뉴럴넷을 직접 써볼건데 한번은 감도학습, 한번은 self-organizing map이라고 잘
안쓰는거지만 해볼거에요. 프로그래밍은 직접 안해도 돼. 이미 있는 툴을 그냥
쓸거에요.

학기 말에 각자가 한 프로젝트를 학회 하듯이 발표를 해. 보고서도 간단히 내는데,
포스터를 벽에 붙이고 발표를 할거에요.

내 과목이 좀 빡빡해요. 빡빡한데, 끝까지 버티면 성적이 나쁜건 아냐. 그러니 빨리
도망갈사람 도망가시고, 버티는 사람들이 약간 절대평가 성격으로 갑니다.

##### 뉴럴넷 학습시킬때 데이터셋 정해주시나요?
정해줍니다. 어느정도까진 보통 하는걸로하고, 나머지는 열어둡니다.

##### 프로젝트 주제는 완전히 자유인가요?
아뇨 꼭 그렇지는 않아요. 교육이 목적이니까 툴을 명확하게 하나로 정하고 데이터는
반반정도.

머신러닝이 실험할때 파라미터들이 좀 있어요. 틀을 정해줘도, 파라미터를 다양하게
바꿔가면서 할 수 있어요. 이과목만 가지고 여러분들이 시간을 다 쓸 수 없다는건
알긴 하는데, 여러분이 이거에 시간을 좀 쓸 생각을 하셔야해요.

이번학기에 크게 하는게 감독, 무감독 학습이에요. K-means, 클러스터링을 공부할거고
프로젝트 두번째로 할것이 Self-organizing map, 그 다음이 베이지안 네트워크. 그
다음이 하이퍼넷.

그다음 내가 까먹기전에 공지할게 있어요. 다음주에 내가 일이 있어서, 보강을 미리
할게요.

*월요일 19~22시*

여기 비디오를 하나 볼게요. 소리가 안나네용. 마이크를 설치해야하는구나. 그냥 보세요.

일반적인 것들은 입력이 들어가면, 알고리즘을 거치면 결과로 데이터가 나와요.

하지만 머신러닝은 데이터가 들어가면 머신러닝을 통해 알고리즘이 결과로 나와요.

시행착오를 반복하고, 경험으로부터 스스로를 향상시켜요.

학습에도 여러가지 종류가 있어요

- 감독학습
- 무감독학습
- 강화학습

강화학습은 잘하면 당근을 주고 못하면 채찍을 주는거에요 찰싹찰싹!  동물을 훈련하듯이.

그리고 감독학습은 우리가 많이 하는건데 정답을 알려주는거에요. 예를들어 문자인식.
직선은 1이다, 이건 2다, 이걸 다 알려주고 배우게 시키는거에요.무감독 학습은

데이터만 주고 아무것도 안해. 기계가 알아서 스스로 패턴을 찾도록 하는거에요.

숫자를 다 섞어놓고 온갓 다 섞어놓고 무감독학습 알고리즘을 돌리면 1자는 1자끼리
묶어주고 2는 2끼리 묶어주고 이런 현상이 발생해요.

#### 기계학습의 정의
환경 E 아래에서, 데이터 D로부터 모델 M을 만드는 알고리즘. 성능을 평가하는 P가
있어요.

#### 왜 기계학습인가?
* 프로그래밍이 어여룬 문제들
  * No explicit knowledge
  * 사람이 직접 코딩해주기 힘듬
  * 환경이 계속 변함
* IT 환경의 변화
  * 빅데이터
  * 컴퓨터 파워
  * 모바일 서비스
* 직접적인 비즈니스 가치 창출
  * 유저 특화
  * 광고
  * 추천

내가 박사때만해도 머신러닝 하는사람 별로 없었는데, 20~30년이 지나는 사이에
기초연구에서 산업이 되었어. 왜 그러느냐? 빅데이터 덕분이야.

컴퓨팅파워, 20년전에도 모자라고 지금도 모자라. 메모리도 모자라. 20년 후에도
모자랄거야.

#### 기계학습의 역사
(피피티 참고)

20년전엔 속도가 달려서 거의 SVM을 써왔음.

역사가 1950년부터 2005년까진 한산한데, 2005년부터 2015년까지 엄청 빽뺵함.

**2012: Deep learning breakthroughs (Google, MS)**

2015: 일론 머스크가 유용하고 안전한 AI를 만들라고 100억을 도네이션.

DARPA 무인차 경주대회. 2014년에도 했는데 단 한팀도 완주를 못했다가, 2015년엔
모두가 성공. 스탠포드 AI랩이 우승했음. 세바스찬, 나랑 같이 공부했던사람.
이게 경쟁은 커녕, 10시간동안 완주하는것도 힘듬.
1등을 하려다보니, 추월도 하고 그럼.

시리. 처음엔 머신러닝 기술이 들어갔지만, 머신러닝을 쓰진 않았는데 이젠
머신러닝을 쓰고있음.

2007년, IBM 왓슨. 위키피디아랑 웹에 있는 텍스트를 싹다 학습시킴.

#### Human-Level Face Recognition
페이스북 얼굴인식하는 뉴럴넷. Deep-face

100만개의 이미지의 클래스를 나누는 AI. 구글.

#### 음성인식 (Acoustic Modeling)
지금까진 음성 노이즈 제거하고, 자꾸 뭐하고, 뭐하고 엔지니어링으로 해결해왔는데

이젠 그냥 raw 데이터를 다 때려박아서 학습시켰더니 훨씬 잘나옴.

#### Movie Recommendations in Netflix
넷플릭스랑 아마존에서 오래전에 상금을 걸고 알고리즘 컴피티션을 열고 그랬음.

> 9월 3일

```
@김젼	출석을 다부르기엔
@김젼	우ㅇ으으으읔
@김젼	출석을 빨리 한번만 불러볼게용
@sgk	중계부탁
@김젼	빨리부릅니다
@김젼	이태석
@김젼	박상률
@김젼	sgm
@김젼	(?)
@sgk	그런거말고
@김젼	다음시간엔 강의실 바꾼대요
@김젼	박성원
@김젼	예에!
@김젼	오늘은 여기까지만 부를게요
@김젼	아래에 107호 비어있는데 옮길까요?
@김젼	네
@김젼	sgk: 오셈
@sgk	옮김?
@sgk	수업자료를 보고 결정해야지
@Gallen	에구 우리애
@Gallen	들으려고 했던 피엘 수업이 팀플이 있나본데
@Gallen	도강하는 입장에서 팀플은 좀 ㅁㄴㅇㄹ이라
@Gallen	그냥 나랑 책 보고 공부하자고 했는데
@sgk	오늘은 그냥 중계만 들어도 되는 수준같으니
@sgk	다음시간부터 열심히 들어야지
@Gallen	SICP 번역본 보면 적당하려나
@김젼	자
@뀽	마의 강의록
@김젼	이 강의실이 딱 맞네
@Gallen	zㅏ
@김젼	오늘의 주제는
@뀽	마의 렉처노트 보세요
@김젼	머신러닝이 무엇이냐를
@김젼	formal하게 다뤄볼거에요
@김젼	....
@sgk	sicp 번역본 추천
@Gallen	저도 서른살이 되기 전에는 그걸 다 읽어보려고 하긴 했거든요
@sgk	역시 사람이 pl을 공부하려면 리습을 써야지
@Gallen	이참에 같이 알려주면서
@Gallen	보는게
@Gallen	헉 알려준대 내 주제에..!
@Gallen	흠흠
@김젼	머신러닝을 크게보면
@김젼	Function approximation
@김젼	또 하나는
@Gallen	어제는 오일러를 한 7~8번까지 풀게 시켰는데
@김젼	펑션 어프록시메이션이
@김젼	약간 결정적인 모델이고
@김젼	반대편 하나는
@김젼	Probability estimation
@김젼	이 두가지를
@Gallen	제가 '오일러'를 풀라고 시킨 건 아니고 어디서 누가 추천을 해줬다길래 봐줬는데
@김젼	이야기해보겠어요
@김젼	하나는 함수근사
@김젼	또 하나는 확률문제
@Gallen	오일러가 프로그램을 더 잘? 예쁘고 똑똑하게? 짜는데에는 별로 도움이 안되는 거 같아서
@김젼	이 두개의 관계를 이해해야돼요
@김젼	무슨 머신러닝 알고리즘이 나와도
@Gallen	앗 필기 다 하면 말해야지
@김젼	이 두개를 알지 않고
@김젼	넘어가기 어려워요
@김젼	>ㅅㅇ
@김젼	글자를 써요
@김젼	2
@김젼	2
@김젼	(서로 다른모양의 2들)
@김젼	2
@김젼	2
@김젼	홍진호
@김젼	콩
@김젼	2
@김젼	그리고 여기에다가는 3을 써요
@김젼	3
@김젼	3
@김젼	등등등등
@김젼	데이터가 주어져요
@김젼	머신러닝이라는거는
@김젼	이 많은 데이터
@김젼	답을 가르쳐줬죠?
@김젼	문제 - 답, 문제 - 답
@김젼	이 엄청나게 많은 문제 - 답 쌍을 만들어놓고는
@김젼	내가 새로 문제를 하나 딷ㄱ 내면
@김젼	이거 답이 뭐냐
@김젼	이거를 맞추는거에요
@김젼	전형적인 패턴인식이죠?
@김젼	스마트폰에서
@김젼	필기인식 해봤으면 알겠지만
@김젼	아직 완전하지 않아요.
@김젼	숫자는 잘 돼요
@김젼	근데 한글, 일반적인 글자 등 하며는
@김젼	완전하진 ㅇ낳아요
@김젼	이게 감독학습? 무감독학습?
@김젼	(감독학습이용)
@김젼	지금 여기엔
@김젼	문제들에 레이블이 달려있는데
@김젼	레이블이 없으면
@김젼	무감독학습.
@김젼	그러면 데이터를 어떻게 표현하느냐
@김젼	보통 우리가 계속
@김젼	머신러닝에서 계속 생각할 식은
@김젼	이거에요
@김젼	답이
@김젼	X 벡터라고 생각하자
@김젼	이 그림이
@김젼	이 그림을
@김젼	비트맵으로 표현해요
@김젼	(0, 0, 0, 0, ....... 1, 0) <- 이게 문자 그림
@김젼	X = (0, 0, 0, 0, ....... 1, 0) <- 이게 문자 그림
@김젼	Y는
@김젼	문제의 답.
@김젼	이 그림과 답을
@김젼	어떻게 X, Y로 인코딩하느냐도
@김젼	중요한 문제에요
@김젼	인코딩하는 방법에 따라 답이 잘 나올수도 있고 아닐수도있어요
@김젼	보통은 벡타형태로 표편해요
@김젼	0부터 9까지의 숫자를 어떻게 벡터로 표현할까?
@김젼	1 of N 인코딩을 쓰자
@김젼	만약에 익
@김젼	이게
@김젼	2다! 하면
@김젼	(0, 0, 1, 0, 0, 0, 0, 0, 0, 0)
@김젼	0, 1, 3~9일 확률은
@김젼	0이고
@김젼	2일 확률이
@김젼	100%
@김젼	이걸 1-of-n 노테이션이라고 그래요
@김젼	트레이닝 데이터는
@김젼	이러한 X와 Y들의 순서쌍 (X,Y)e들의 집합이에요
@김젼	트레이닝 데이터 D = {(Xi, Yi) | i = 1, ..., N }
@김젼	 N 은 데이터의 수
@김젼	90년대 초반부터
@김젼	이러한 OCR 트레이닝 데이터를 꾸준히 모아오고
@김젼	컴피티션을 하고 그랬어요
@김젼	내가 공부할때엔 94~95% 성공률이었어요
@김젼	많은 AI 문제가
@김젼	성능이
@김젼	100%에 가까워질수록
@김젼	1% 성능을 향상시키기가
@김젼	점점점 힘들어져요
@김젼	90%까지는
@김젼	금방 가
@김젼	근데 90%에서 95%로 가기는 정말 힘들고
@김젼	95%에서 97.5%로 가기는 더더욱 힘들어요
@김젼	갈수록 상당한 시간이 드는데
@김젼	이 시간이라는게 1~2년이 아니고
@김젼	1950년대에
@김젼	AI 기술이 생겼을떄
@김젼	그당시 사람들은
@김젼	1980년대면 컴퓨터가 사람이랑 말을 자유롭게 할거라고 생각했어요
@김젼	근데 2015년에 와서도 잘 안되죠?
@김젼	그래요
@김젼	온르은
@김젼	오늘은
@김젼	감독학습 위주로
@김젼	이야기를 할게요
@김젼	요거를 뽀말하게 표시를 하면
@김젼	Xi = (x1, x2, x3, ..., xn)
@김젼	Yi = (y1, y2, .., ym)
@김젼	인풋(X)은 n차원의 바이너리, 실수 벡터
@김젼	그리고
@김젼	내가 에러를 정의해요
@sgk	이상하다
@sgk	수업내용이 ppt랑 다르다
@김젼	E = Sigma | yd - f(xd * w) |
@김젼	PPT 안봄
@sgk	뭑
@김젼	E = Sigma | yd - f(xd * w) |^2   (d = 1 ~ N)
@김젼	f(xi * w) 가 뭔지
@김젼	설명해드릴게요
@김젼	그냥 샘플하는 함수에요
@김젼	f는
@김젼	결과로 y가 나오는거에요
@김젼	w는 뭐냐면
@김젼	f는 모델
@김젼	학습이 뭐라그랬어요
@김젼	데이터로부터 모델 만드는거라그랬어요
@김젼	D => M
@김젼	w가 모델이 쓰는 파라미터에요
@김젼	f는 모델이고.
@김젼	예를들어
@김젼	f가 뉴럴넷이다 치면
@김젼	뉴럴넷 사이에서 쓰는 역전파속도 등등등 모오든 파라미터가 w라고 생각하면 돼요
@김젼	f가 디시젼트리다 치면
@김젼	w는 좔좔좔좔이에요
@김젼	그래서
@김젼	E = Sigma | yd - f(xd * w) |^2   (d = 1 ~ N)
@김젼	이게 아주 제너럴한 정의에요
@김젼	가장 심플한 정의에요
@김젼	원하는 목표치가 yd
@김젼	실제 출력치가 f(xd * w)
@김젼	이거의 차이를
@김젼	제곱합한것이
@김젼	에러인거에요
@김젼	문자인식으로 돌아가면
@김젼	내가 뉴럴넷을 만들어서 트레이닝을 열심히 했어
@김젼	보통
@김젼	문자인식을 하면
@김젼	얘가 1이랑 얼마나 닮았는지, 2랑 얼마나 닮았는지 정도가 실수로 나와요
@김젼	이 숫자가 2면 내가 원하는값은
@김젼	(0, 0, 1, 0, 0, ~) 인데
@김젼	실제로 나오는 값은
@김젼	(0.1, 0.1, 0.9, 0.1, ~~) 이럴거 아냐
@김젼	이거를 빼서 제곱합하면
@김젼	에러가 되는거징
@김젼	학습이라는건 뭐냐면
@김젼	이러한 상황에서
@김젼	Find f(-;w) such that y = f(x;w)
@김젼	            or minimizing E_N
@김젼	이게 학습의 정의에요.
@김젼	                   Given D
@김젼	y = f(x;w)
@김젼	정확하게 값이 같아지는
@김젼	f w 를 찾는게
@김젼	머신러닝의 꿈이에요
@김젼	이건 이상이고
@김젼	그럴 수 없으니
@김젼	E를 최소화하는거에요
@김젼	이것이 바로
@김젼	Function approximation이에요.
@김젼	아주 쉬운말로 생각을 해보자
@wook	btzhang 수업?
@김젼	네
@김젼	그래프를 하나 그리고
@김젼	가로축이 애긔들의 나이
@김젼	한살 두샬 세샬
@김젼	세로축이
@김젼	애긔들의 키
@김젼	라고 생각해보자
@김젼	데이터포인트가
@wook	한글수업인가보네
@김젼	이렇게 뚝뚝뚝 주어져있어요
@김젼	예를들어
@김젼	두살짜리 애가
@김젼	몇센치지?
@김젼	한 70센치라고 쳐봐
@김젼	아니다 50
@김젼	그리고
@김젼	다섯살짜리애가 80센치
@김젼	이랬다고 쳐보자
@김젼	이게 데이터셋이죠? (그래프 위에 무수한 점들)
@김젼	x1 = (2yr, 50cm)
@김젼	x2 = (3yr, 55cm)
@김젼	x3 = (2yr, 52cm)
@김젼	이걸
@김젼	숫자로 그대로 써도 되지만
@김젼	우리가 이해하기엔
@김젼	그냥 이렇게 2차원짜리 인티저로
@김젼	쓰는게 쉽죠?
@김젼	근데 바이너리 벡터로 펼치는게
@김젼	머신러닝이 더 나아요
@김젼	그게 사실
@김젼	지금
@김젼	머신러닝 뉴럴넷들을
@김젼	사람들이
@김젼	그냥 블랙박스처럼 쓰잖아요?
@김젼	툴 주면 딱 쓰고
@김젼	근데 내가
@김젼	머신러닝을 알아야 한다는 말이
@김젼	이런 입출력을
@김젼	어떻게 인코딩할건지
@김젼	그런 노하우를 알아야돼요
@김젼	아 미아
@김젼	미안
@김젼	저거 저렇게쓰는거 아냐
@김젼	(x1, y1) = (2yr, 50cm)
@김젼	(x2, y2) = (3yr, 55cm)
@김젼	고마워요
@김젼	제대로 지적했어요
@김젼	머신러닝이라는건
@김젼	x가 주어지면
@김젼	y를 찾는 문제에요
@김젼	그래서 내가
@김젼	이 점들사이에
@김젼	추세선을 그렸다고 쳐봐
@김젼	데이터가 없는데
@김젼	선을 쭉 그었어
@김젼	쮸욱
@김젼	그럼 예측을
@김젼	요값으로 예측할 수 있죠?
@김젼	이게 바로 예측이에요
@김젼	이거
@김젼	전혀 새로운 문제가 아니죠
@김젼	고등학교떄부터 한거에요
@김젼	뻥셔널 인터폴레이션
@김젼	인터폴레이션이 한글로 뭐지?
@김젼	나: 보간이요
@김젼	보간
@김젼	이게 수학에선
@김젼	몇백년된 문제죠?
@김젼	함수를
@김젼	근사하는거에요
@김젼	머신러닝은
@김젼	아주 복잡한 모양의
@김젼	이 임의의 함수를
@김젼	근사하는거에요
@김젼	수학에서는
@김젼	다항식을 쓰지?
@김젼	폴리노미얼 인터폴레이션 하면서
@김젼	1차 다항식으로 근사할수도있고
@김젼	3차다항식으로 근사할수도 있고
@김젼	여튼 공부를 하다보면
@김젼	이런 뻥셔널 어프록시메이션이 종종 나올거에요
@김젼	명확하죠?
@김젼	머신러닝이 뭘 하려고하는건지
@김젼	근데 요렇게만 설명하면
@김젼	머신러닝을 왜 해야하는지
@김젼	이해가 안되겠죠
@김젼	이거
@김젼	수치해석에서 많이 나온 테크닉이에요
@김젼	그래서
@김젼	지금 머신러닝이
@김젼	과거에 있던 통계학이나 수학을
@김젼	쓰기도 해요
@김젼	근데 이걸로 끝나지 않아요
@김젼	이건 아주 작은 시작이죠
@김젼	예를들어 수학은
@김젼	이런걸 아주 간단하게만 어프록시메이션할라그랬는데
@김젼	딥러닝에선
@김젼	굉장히 복잡한 모양의 커브를 만들지
@김젼	여튼 요게 뻥셔널 어프록시메이션으로 본 감독 학습이었어요
@김젼	무감독 학습은 모냐!
@김젼	쓱싹쓱싹
@김젼	무감독 학습은
@김젼	똑같은 데이타아
@김젼	어 그니까
@김젼	어 음
@김젼	데이타
@김젼	요거를 좀 지우고할게요
@김젼	무감독 학습을
@김젼	뻥셔널 어프록시메이션으로 본다면
@김젼	데이터가 저런식으로 안주어지고
@김젼	이렇게 주어져요
@김젼	y가 없어
@김젼	D = { xd | d = 1~N }
@김젼	E는 어떻게 정의하냐면
@김젼	E_N = Sigma | xd - f(xd;w) |^2
@wook	오토인코더!
@김젼	     = Sigma SIgma ( xk(d) - fk(xd;w) )^2
@김젼	타자로 못치겠졍
@김젼	f는
@김젼	f :: Vector
@김젼	f = (f1, f2, f3, ... ) 이렇게
@김젼	f도 출력값이 벡터라서
@김젼	차원에 따라 잘라서
@김젼	f1 f2 이렇게 표현할 수 있거든요?
@김젼	음
@김젼	벡터합
@김젼	제곱한걸 굳이 설명안해주셔도
@김젼	될거같은데
@김젼	*벡터차 제곱한거
@김젼	무감독 학습은 y가 없어용?
@김젼	에라함수를 어떻게 정의하냐면
@김젼	입력을 줬쬬?
@김젼	입력하고 출력이 같아요
@김젼	내가 입력주고 f라는 함수를 만들고 출력을 바라는데
@김젼	출력이 입력하고 똑같길 바래
@김젼	이걸 그냥 카피해버리면 쉽죠?
@김젼	근데 학습이 안일어나지
@김젼	그래서 f를 만들어내야돼
@김젼	그래서 무감독학습이란 뭐냐면
@김젼	Find f(-;x) such that x = f(x;w)
@김젼	             or inimizing E_N, Given D
@김젼	그래서
@김젼	애니를 열심히 학습시켜서
@김젼	애니를 안볼때에도 덕질을 하는 AI를 만드는거시다
@김젼	뽀롱뽀롱뽀로로
@김젼	이 에라를 줄이는것이
@김젼	머신러닝이 하는일이야
@wook	무감독학습의 한 분류로 저런 걸 하는건 맞는데 무감독학습은 그게 다인것처럼 이해될 우려가 ㅠㅠ
@김젼	머신러닝의 분류는
@김젼	f를 어떻게 만드느냐
@김젼	로 분류할 수 있고
@김젼	f하고 W의 모양이 어떻게 되느냐
@김젼	도 많이 달라져
@김젼	또 이제 할얘기는
@김젼	감독학습과 무감독학습의
@김젼	관계에요
@김젼	이거 아주 중요해용
@김젼	이 둘은
@김젼	아주 밀접한 관계가 있어요
@김젼	이미 눈치챘겠지만 ㅇㅅㅇ)+
@김젼	감독 vs 무감독
@wook	ㅋㅋ
@김젼	내가
@김젼	z라는 벡터를 만들게요
@김젼	z = (x, y)
@김젼	그러면 트뢰이닝 데이타가 어떻게 주어지냐면
@김젼	요번엔 z라는걸로 묶어서 생각하니까
@김젼	D = { Zd = (Xd, Yd) | d = 1~N }
@김젼	아 그리고 지금
@김젼	제곱합 계속 쓰니까
@김젼	제곱합 연산자좀 정의할께요
@김젼	이거 타이핑 어떻게해
@김젼	ㅇㅅㅇ
@김젼	님들 빨리
@김젼	제곱합함수
@p	아으 죽겠네
@김젼	아이디어좀
@p	제곱합함수?
@wook	..?
@김젼	sum of squared error
@wook	SSE?
@p	sigma a_n^2 ?
@p	아하
@김젼	우왕
@p	sse로 합시다
@김젼	E = SSE(Z - f(z;w))
@김젼	요렇게 되겠죠?
@김젼	고다음메
@p	streaming simd extensions
@김젼	sse
@김젼	Find f(-;w) such that z = f(z;w)
@김젼	        or minimizing E_N, Given D
@김젼	자 이렇게 감독학습을 정의해보아써요
@김젼	자네 펜하나만 좀 빌려주라
@김젼	자네 이름이 머였지
@김젼	(뭔가 적음)
@김젼	수업노트 고치시는듯
@김젼	자 이렇게
@김젼	뽀뮬레이션하면
@김젼	감독학습이
@김젼	무감독학습으로 변신했죠?
@김젼	그냥 x대신 z로 바꾼거자나
@김젼	ㅇㅅㅇ+
@김젼	그냥 z를
@김젼	입출력을 합친
@김젼	(X, Y)로 만들고
@김젼	무감독학습으로 만들었찌
@김젼	자 이렇게
@김젼	감독학습과 무감독학습, 무감독학습모양으로 만든 감독학습
@김젼	세개를 보자
@김젼	뽀롱뽀롱
@김젼	뽀로로로롱
@김젼	크롱
@김젼	이
@김젼	내가 제일
@김젼	맨 마지막에 보여준게
@김젼	유니버셜 레프리젠테이션이에요
@김젼	감독 무감독을 나눌 필요도 읎어
@김젼	주어진 데이터를
@김젼	리컨스트럭션한 알고리즘을
@김젼	만들면
@김젼	이건 이론적으로 유니버셜해요
@김젼	실제로 잘하느냐와는 달라
@김젼	이론적으로 범용적이면 프랙티컬하게는 범용성이 큰경우가 많아
@김젼	여튼 이 맨 왼쪽 포뮬레이션으로 만들면 유니버셜하고
@김젼	내가 이론적으로도 권장해요
@김젼	그리고 내가 한학기동안 가르칠게
@김젼	이거에요
@김젼	*맨 오른쪽 포뮬레이션
@김젼	f(z;w)
@김젼	지금 우리가
@김젼	블랙박스를 생각해보면
@김젼	f 를
@김젼	x는 인풋을 넣으면
@김젼	y가 나오는걸 생각했죠?
@김젼	y = f(x;w) :: 감독학습
@김젼	무감독학습을
@김젼	생각하면
@김젼	x하고 y를 다 넣고
@김젼	x하고 y가 나오길 기대하는거에요
@김젼	z = f(z;w)
@김젼	근데 여기서
@김젼	y를 생각하는게
@김젼	필요는 한게
@김젼	트레이닝할때엔
@김젼	x,y를 붙여서 하겠지만
@김젼	우리가 실제로 써먹을때엔
@김젼	y가 없고 x만 있잖아?
@김젼	얘가
@김젼	내부적으로
@김젼	조인트 프로바빌리티 (뭔지모름)이 있어서
@김젼	x없이 y를 만들어낼 수 있고
@김젼	거꾸로 y만 주고
@김젼	x를 만들어낼 수 있어요
@김젼	우리가 기계를 상상을 못한다고 그러죠?
@김젼	근데 이제 뉴럴넷으로 학습시켜보면
@김젼	3을 상상해보세요
@김젼	(3의 이미지)
@김젼	이게 가능해지는거에요
@김젼	이론적으로 가능하고
@김젼	프랙티컬하게도 그런 시도가 있어요
@sgm	필사쟁이네
@김젼	꾸쯋
@wook	joint probability는 p(x,y) 를 의미함
@wook	p(y) = \sum_x p(x,y)
@wook	etc.
@김젼	학습과
@wook	만화이해왕
@김젼	퍼포먼스 단계를
@wook	'ㅅ'
@김젼	나눌수도있어요
@김젼	학습이
@김젼	논리게이트와 제일 다른게
@김젼	그거에요
@김젼	우리가 로직게이트는 바이너리만 입력으로 주고
@김젼	바이너리만 출력으로 나오잖아?
@김젼	0하고 1
@김젼	우리가
@김젼	학습을 할때엔
@김젼	바이너리 입력 써도 돼
@김젼	입력이 실제로 그러하다면 바이너리 입력 써도 돼
@김젼	근데 출력은 0과 1 사이의 아날로그로 나와요
@김젼	아날로그 컴퓨터를 만드려는것에 가까워요
@김젼	입력은
@김젼	이산적이지만
@김젼	출력은
@김젼	커브가 나와요
@김젼	그래서 상당히
@김젼	폴트 톨러런스하고
@김젼	어답티브해요
@김젼	로버스트해요
@김젼	주어진 데이터는 discrete한걸 쓰는데
@김젼	학습한건 임의의 노이즈가 있어도 돼요
@김젼	우리가 접하는 데이터는
@김젼	상당히 finite해요
@김젼	학습모델이 생성하는 결과는
@김젼	상당히 robust하고
@김젼	general해요.
@김젼	우리가 아직 첫시간이라서
@김젼	안한게 있는데
@김젼	내가
@김젼	f(z;w)이렇게
@김젼	미니마이즈했잖아요?
@김젼	근데 이건 좋은 답이 아니에요.
@김젼	내가 일부러 얼버무렸는데
@김젼	x가
@김젼	트레이닝 데이터 D만을 의미하는게 아냐
@김젼	인풋 스페이스에 있는
@김젼	관측하지 않은 데이터까지 포함하여
@김젼	모든 가능한 x=f(x;w) 에요
@김젼	minimizing E_N만 하면
@김젼	이건 오버피팅하게되어요
@김젼	나중에 레귤러라이제이션을 배울거에요
@김젼	우리가
@김젼	x=f(x;w)를
@김젼	카피함수로 못만드는게
@김젼	이 이유때문이에요
@김젼	학습은
@김젼	GEneralization을 하는 알고리즘이에요
@김젼	일반화해야돼
@김젼	일반화한다는게 뭐냐면
@김젼	트레이닝 데이터 N개를 학습시켰지만
@김젼	나중엔 N이 무한이 되어도.
@김젼	모든게 되어도
@김젼	E가 미니마이즈되길
@김젼	바라는거에요
@김젼	로봇축구 생각하면 쉽죠?
@김젼	축구경기장에 나가면
@김젼	온갖 상황이 벌어질텐데
@김젼	모든 상황에서도 얘가
@김젼	메시처럼 축구를 잘 하길 바라는거지
@김젼	일반화가 아주 중요한 특성이에요
@김젼	내가 수업하는거
@김젼	카메라로 찍어도 돼요
@김젼	조교가 칠판 내가 지우기 전에 좀 찍어봐요
@김젼	다른사람도 찍어도 돼용
@sgk	찍어라 김젼
@sgk	보내줘
@wook	ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ
@김젼	시렁
@wook	쿨하시네 장교수님
@김젼	초등학생 아니니까
@sgk	ㅠㅠ
@김젼	알아서들 잘 하세요
@김젼	자 그럼
@김젼	뻥셔널 어프록시메이션은 여기까지 하고
@김젼	지금까지 한 이야기들은 다
@김젼	디터미니스틱한 이야기에요
@김젼	두번쨰 정의에선
@김젼	랜덤 배리어블이 나와요
@김젼	확률모델
@p	ㅋㅋㅋㅋ
@김젼	지금 첫번째껀
@김젼	입력이같으면
@김젼	출력이 항상 같은경우였지?
@김젼	근데 두번째의 경우엔
@김젼	입력이 같아도
@김젼	출력이 달라질 수 있어
@sgm	nondeterministic 이야기 아닌가?
@김젼	그란도시즌을
@김젼	들어봐요
@김젼	하나둘셋이에요
@김젼	그란도시즌이에요?
@김젼	똑같은 입력을 줬는데
@김젼	해석이 다른거야
@김젼	달라질쑤이써
@김젼	특히 뭐
@김젼	사람이 번역한다 그러면
@김젼	맥주한잔 한거랑
@김젼	커피마시고 밤새고 한거랑
@김젼	다를수있지?
@김젼	확률모델이
@김젼	아주 흥미로운 모델인데
@김젼	믈논 장단점이 있어요
@김젼	그래서 대부분의 실제적인 모델에선
@김젼	확률모델이 잘 안쓰여
@김젼	근데 연구자들은 머신러닝에서
@김젼	확률모델을 많이 연구해왔어요
@김젼	확률모델 정의를 해볼텐데
@김젼	끝내긴 어렵고
@김젼	가다가 말게요
@김젼	자 그래서
@김젼	똑같은
@김젼	요거요거를 씁시다
@김젼	자 X와 Y라는건 알아쓰요
@김젼	감독학습을
@김젼	확률모델로 바라보면
@김젼	감독학습이 뭐냐면
@김젼	이 확률구하는 문제에요
@김젼	조건부 확률
@김젼	P(Y|X)로
@김젼	포뮬레이션할 수 있어요
@김젼	또 이걸
@김젼	Z=(X,Y)입출력을
@김젼	합쳐서 생각하면
@김젼	쪼인트 프로바빌리티 에스티메이션 하는 문제로 변해요.
@김젼	감독 : P(Y|X)
@김젼	무감독 : P(Z) = P(X,Y)
@김젼	쬬쬬
@김젼	두개의 관계는
@김젼	어떻게돼요?
@김젼	두개의 관계는 어떻게 돼요.
@김젼	그거를 알라면
@김젼	확률의 기본법칙을 한번 알아볼까용?
@김젼	확률에 기본법칙
@김젼	간단한거 두개밖에 없어요
@김젼	그냥 우리
@김젼	 초등학교때부터 배우는
@김젼	셈하는 방법
@김젼	알제브라
@김젼	배수법에
@김젼	보통 뭐배워요 우리가
@김젼	부울대수 이런거 배울떄
@김젼	가장 기본적으로 배우는게 뭐에요
@김젼	덧셈뺄셈
@김젼	확률
@김젼	곱의 법칙
@김젼	1) Product rule
@김젼	쪼인트 프로바빌리티에요
@김젼	P(X,Y) = P(X) * P(Y|X)
@김젼	       = P(Y) * P(X|Y)
@김젼	이게 곱의법칙이고
@김젼	두번째는 합의법칙이에요
@김젼	2) Sum rule
@김젼	썸룰
@김젼	썸남썸녀
@김젼	<@wook> p(y) = \sum_x p(x,y)
@김젼	정확히 같네
@김젼	이렇게 쓰면
@김젼	x가 사실 필요 없어져
@김젼	우리가
@김젼	무감독 학습을 해서
@김젼	P(x,y)를 구했어요
@김젼	동전이랑 주사위를 계속 던져서
@김젼	확률을 잘 구했어
@김젼	근데
@김젼	우리가 그냥
@김젼	주사위 눈이 6만 나올 확률이 궁금해
@김젼	그러면
@김젼	동전 앞일떄 6확률 + 동전 뒤일때 6확률
@김젼	하면 되지?
@김젼	이게 아주 뻔대맨탈한거에요
@김젼	P(Y)는
@김젼	우리가 궁금한
@김젼	알고싶은 확률이에요
@김젼	그리고
@김젼	X는
@김젼	우리 머릿속의
@김젼	숨은 변수에요
@김젼	Latent variable이라고 그래요
@김젼	자고있는 변수
@김젼	머신러닝은
@김젼	X를
@김젼	자동으로 찾는 알고리즘이지
@김젼	관측된 거대한 Y가 있는데
@김젼	모르는 뭥가가 이써
@김젼	눈에 보이지 않는 뭔가가 이써
@김젼	허블망원경으로 우리가 메져하는건 Y에요
@김젼	근데 우쥬에 무슨 숨은 법칙이 있는거같애
@김젼	그게 멀티룰 X에요
@김젼	기계의 힘을 빌려서
@김젼	모든 숨은 X의 가능성을 찾는거에요
@김젼	너무 철학적으로 얘기했나?
@김젼	근데 그게 진짜에요
@김젼	그정도로 생각하고 머신러닝을 해야돼
@김젼	그렇지 않으면
@김젼	뻥션 어프록시메이션
@김젼	이거 수치해석에 다 있는 기술이야
@김젼	이게 두개고
@김젼	3은
@김젼	앞의 두개로 유도가능한데
@김젼	중요해서 한번 더 말하자면
@김젼	베이스 룰이에요
@김젼	3) Bayes rule
@김젼	머신러닝에서 베이지안 얘기해요
@김젼	베이지안을 모르면 머신러닝을 모르는사람 취급받을거에요
@김젼	베이지안이 약간
@김젼	철학이 있어요
@김젼	통계 내에서 철학이 좀 들어가는데
@김젼	주관이 들어가
@김젼	그래서 통계학자들이
@김젼	베이지안을 최대한 배제하려고했는데
@김젼	결국 포기하고
@김젼	지금은 베이지안이 아주 널리 쓰여요
@김젼	내가 공부할때엔 베이지안이 많지는 않았어요
@김젼	아 이거
@김젼	알아
@김젼	P(Y|X) = P(X|Y) * P(Y) / P(X)
@김젼	P(X|Y) = P(Y|X) * P(X) / P(Y)
@김젼	이 베이스 룰은
@김젼	그냥 1에서 한줄이면 유도할 수 있어요
@김젼	근데
@김젼	의미는 아주 심오해요
@김젼	수식에만 사로잡혀있으면 안되고
@김젼	의미를 생각해야돼요
@김젼	P(Y|X)는
@김젼	X가 관측이고
@김젼	Y가 예측이라고 생각하면 돼
@김젼	X가 입력이고
@김젼	Y가 출력이라고 생각하면 돼
@김젼	머신러닝은 많은것이 예측 알고리즘이에요
@김젼	과거로부터
@김젼	미래를 내다보는 알고리즘이에요 다
@김젼	프로그램도 다 그런거죠?
@김젼	여러분들 짜는 프로그램 다
@김젼	과거로부터 미래를 하고싶은거에요
@김젼	근데 이 프로그램이
@김젼	나중에 쓰이는거는
@김젼	문제풀때 쓰이는거는
@김젼	어떤문제를 풀지 아는거는 어려워요
@김젼	근데 머신러닝은
@김젼	현장을 갔을때
@김젼	모르는 상황이 많을수록
@김젼	더 위력을 발휘하죠
@김젼	머신러닝은
@김젼	미래를 예측하고싶은거에요
@김젼	알고있는 미래에 대비하는게 아니라
@김젼	관측된 데이터를 가지고
@김젼	에러만 미니마이제이션하면
@김젼	과거데이터에 굉장히 연연하게되어요
@김젼	핵심은 모르는 데이터가 들어왔을떄 하는거지
@김젼	베이지안은
@김젼	출력을 알때
@김젼	입력을 예측할 수 있어
@김젼	굉장히 재밌는 부분이에요
@김젼	내가 저거를 설명한 진짜 이유는
@김젼	이 식을 설명하기위해서였는데
@김젼	쪼인트 프로바빌리티는
@김젼	감독학습은
@김젼	사실은 요고를
@김젼	호호혿
@김젼	이 식을 쓰기위해
@김젼	오늘 수업을 하여따
@김젼	P(Y|X) = P(X,Y)/P(X)
@김젼	좌변은 감독학습이고
@김젼	우변은 무감독학습이죠?
@김젼	수식으로 보면 너무나도 자명해요
@김젼	kcm노트북 계정이름이 unused네
@김젼	둘이 무슨사이에용
@김젼	내가 무감독학습을 해놓으면
@김젼	감독학습에서 알고자하는걸 예측할 수 있어
@p	그렇고 그런 사이
@김젼	1/P(X) 가
@김젼	노말라이즈하는 텀인데
@김젼	자명한거같지만
@김젼	사실 계산 불가능한 term이에요.
@김젼	내가
@김젼	풍경 영상인식을 하겠다고 쳐봐요
@김젼	산을 찍으면
@김젼	토끼, 나무, 자연
@김젼	모든걸 다 인식할 수 있어야돼.
@김젼	모든 사람 얼굴
@김젼	딱 주면
@김젼	이게 무슨오브젝트인지 다 알아맞추고 싶어해요
@김젼	그러면
@김젼	이럴라면
@김젼	모든 우주의 영상에 대한
@김젼	1/P(X) 를 계산할 수 있어야하죠?
@김젼	근데 불가능해요.
@김젼	보지도 않은걸 무슨수로 상상해
@김젼	이부분은
@김젼	이론적으로는 불가능한데
@김젼	프랙티컬하게 1/P(X)를
@김젼	어프록시메이션하는거에요
@김젼	뭔가 영상을 보면
@김젼	이게 치타인거같기도 하고
@김젼	재규어인거같기도 해
@김젼	그러면
@김젼	확률이 더 높은걸로 어프록시메이션하는거에요
@김젼	그래서 여러 머신러닝 모델들이 다 P(X)를 어프록시메이션하는거에요
@김젼	우리가 다루는 모델들은
@김젼	2/3은
@김젼	다 디터미니스틱한 알고리즘들이고
@김젼	다 E 미니마이즈하는거에요
@김젼	현실떄문에 어쩔수가 없는거지
@김젼	근데 연구하는건
@김젼	확률적인 모델들이에요
@김젼	딥러닝 안에서도 역시
@김젼	디터미니스틱한거랑
@김젼	확률적인게 따로 있어
@김젼	예를들어
@김젼	컨벌루젼 뉴럴넷
@김젼	이런건 디터미니스틱한거에요
@김젼	근데
@김젼	디터미니스틱한것이 이런 확률적인 모델의 서브셋이라서
@김젼	제너럴한 이론적인 형태를 먼저 이해하면
@김젼	도움이 될 수 있어
@김젼	다음시간부터는 여기러 오세요?
@김젼	수업긑
```
