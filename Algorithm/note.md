Week 1, Mon
========
첫수업, 교수님께서 대형강의를 좋아해서 온라인강의에 불만이 많다고 하신다.

알고리즘의 사전적 정의: 입력으로부터 출력을 만드는 과정을 애매하지 않게 기술한 것. 문제해결 = Data structure + Algorithm. 이 강의에선 알고리즘 자체를 공부하기도 하지만, 그만큼 중요하게 생각하는 방법을 익히게 될 것이다. 알고리즘을 공부하는건 현재의 문제를 해결하는법을 배우는것이지만, 생각하는 방법을 배우는 것은 미래의 문제를 미리 해결해두는것이나 마찬가지입니다.

이번 강의에서 다룰 중요한 주제들

1.  알고리즘 설계와 분석의 기초. 알고리즘에 대한 개략적인 이야기를 할것이다. 알고리즘 복잡도에 대해 다루게된다.
2.  Recurrences, 점화식. 재귀와 밀접한 관련이 있다. 재귀적 구조를 가진 알고리즘의 복잡도는 어떻게 계산하는가 이런것도 다룸.
3.  Sorting, 정렬. 자구에서도 했지만 더 깊게 배움. 알고리즘 각각의 설명, 복잡도 계산 및 증명, 각각의 아이디어를 깊게 다룸.
4.  Selection, 선택. "n개의 수 중에서 n/3번째로 작은 원소 찾기" 이런걸 다룸. 이런 알고리즘들을 소개함. O(n^2) 복잡도의 원시적인 알고리즘부터 O(nlogn), O(n) 등 다양한 알고리즘을 배움.
5.  Search Trees. 자구에서 배운것보다 더 깊게 다룸. 자료구조의 이론적 퍼포먼스 증명 등
6.  Hash Tables. 얘도 증명 포함됨
7.  Handling Exclusive Sets. 배타적인 집합을 다루는 방법.
8.  Dynamic Programming. 아무리 강조해도 지나치지 않음. "동적 프로그래밍"이라는 단어는 DP가 실제로 어떤것인지랑은 무관해서 부적합한 표현임. 재귀적인 구조를 가진 문제를 푸는 방법. 아주 중요함
9.  Graph Algorithms. DFS, BFS, 미니멈 스패닝 트리, 최단경로문제, Strongly connected component, 토폴로지 소팅 등 다양한 문제를 다루고, 알고리즘이 왜 맞는지 증명도 함.
10. Greedy Algorithms. 드물게 그리디한 접근을 해도 글로벌 옵티멈을 보장함. 프림 알고리즘, 크루스칼 알고리즘, 다익스트라 알고리즘. 이런 예가 많지 않은데 이런 그리디들을 배움. [매트로이드]라는 수학적인 구조를 가지면 어떤 성질로 도움을 받을 수 있는지, 이런걸 배움. 매트로이드는 그래프의 더 abstract한 mathematical object
11. String Matching. Exact matching을 배우면 Approximate matching 알고리즘도 만들 수 있음.
12. NP-Complete/Hard. 이 앞 챕터까지는 어떤 문제를 잘 풀수있을까에 대한 이야기인데, 이 챕터는 어떤 문제가 풀기 힘드냐를 배우게됨. "현실시간에 풀 수 없다"는 가정을 다들 믿고 살긴 하는데 그것도 증명이 안되어있어서 세계 7대 난제로 등록되어있음.
13. Problem Space Search. 문제를 푸는 Rugged한 공간에서 어떻게 이동할것인가, 어떻게 탐색을 할것인가에 대해 배움. 문제공간의 특성, 문제공간을 어떤 관점으로 보느냐.

[매트로이드]: https://en.wikipedia.org/wiki/Matroid

추천교재

1. 쉽게 배우는 알고리즘, 문병로, 한빛미디어
2. Introduction to Algorithms, Cormen et al, MIT Press.

숙제는 총 4회. 2회는 Handwriting, 2회는 프로그래밍.

보통 알고리즘 강의는 30명정도가 하는데, 지금 알고리즘 수업이 120명이 하는 대형강의가 되었다. 조교들이 본인 연구를 하면서 동시에 숙제 채점을 하기때문에 굉장히 일이 힘들다. 조교들이 대답이 늦더라도 양해를 부탁한다.

시험은 반드시 교실에서 봄. Open Book. 시험시간 들고올 수 있는것은 책 두 권, 강의 노트 프린트+메모, 기출 문제 및 자신이 풀어본 내용. 두 교실로 나눠서 시험을 보게됨. 디지털 미디어는 가져오면 안됨

&nbsp;

Week 1, Wed
========

&nbsp;

## 1. 알고리즘이란
알고리즘: 문제 해결 절차를 체계적으로 기술한것

### 알고리즘을 공부하는 목적

1.  특정 문제를 해결하는 방법을 배우기도 있지만
2.  체계적으로 생각하는 훈련을 하고
3.  지적 추상화 레벨의 상승(Intellectual abstraction)을 이룩할수있다. 연구나 개발에 있어 정신적 여유를 유지하기 위해 매우 중요한 요소다!

문교수님 왈 워낙 다양한 문제를 다루고 시행착오를 겪고 나니까, 새로운 문제 새로운 프로젝트를 다룰때에 "이걸 해결할 수 없으면 어쩌나"하는 느낌을 겪지 **못할까봐** 두렵다. 너무 쉬우면 제대로된 논문으로 나오지도 않고, 남에게 내세우지도 못하고, 자신에게 만족감도 줄수없고.

챌린징한 문제를 푸는 경험을 누적해나가는게 중요하다. 지금 님들한테 주는 과제가 처음 보면 꽤 어려울 수 있는데, 계속 머리에 넣어두고 계속 생각해보면 듀데이트가 될 즈음이 되면 어떻게든 풀게될겁니다. 머리속에 일단 넣어두고있으면 밥먹을때 버스탈때 잘때 잠재의식이 계속 풀고있을거다!

문제 자체를 해결하는 알고리즘을 배우는것도 중요하지만, 생각하는 방법을 배우는게 더 중요하다! 미래에 맞딱드릴 문제를 해결할 빌딩블록을 얻게될거다.

알고리즘은 자료구조의 확장이다. 자료구조는 건축이나 자동차 제작의 건축자재, 부품, 모듈같은것임. 이거를 조립해서 더 복잡하고 어려운 문제를 푸는거다.

&nbsp;

## 2. 알고리즘 설계와 분석의 기초

### 알고리즘 수행시간
바람직한 알고리즘은 명확해야하고, 가능하면 간명해야한다. 지나친 기호적인 표현들은 오히려 명확성을 떨어트린다. 명확하게 기술할 수 있다면 그냥 말로 풀어써도 됨.

바람직한 알고리즘은 효율적이어야한다. 수행시간 효율적이어야 하고 알고리즘이 차지하는 메모리 공간이 효율적이어야함. 이 알고리즘 클래스에선 수행시간에 주로 집중한다.

알고리즘의 수행시간에는 여러가지 클래스가 있다. log N, N, NlogN, N^2, N^3, 2^N, 등.

알고리즘 수행시간이 n^2/5, 5*n 이런식으로 주어진다면 상수때문에 입력이 작을때에는 n^2/5 알고리즘이 더 빠를수도있지만, 입력이 충분히 커지면 차수가 큰 알고리즘이 이기게된다. asymptotic(점근의) notation에서는 계수가 중요하지 않고 차수가 중요하다.

알고리즘의 수행시간은 for 문 반복회수, 특정 행이 실행되는 회수, 함수 호출회수 등 다양한 기준으로 결정할 수 있다.

(예제로 수행시간 계산하는 문제 여럿 풀어봄)

### 재귀와 귀납적 사고
Optimal Substructure: 큰 문제의 최적해가 자신보다 작은 문제의 최적해를 포함하는 구조

예제: 머지소트 `T(n) = 2*T(n/2) + cn`

```
T(n) = 2*T(n/2) + cn
     = 2^1 * T(n/2^1) + 1*cn
     = 2^2 * T(n/2^2) + 2*cn
     = 2^3 * T(n/2^3) + 3*cn
     ...
     = 2^k * T(n/2^k) + k*cn     (k = log_2 n)
     = 2^k * T(1)     + k*cn     (2^k = n)
     = a*n + b*nlogn
```

### 다양한 알고리즘의 적용 주제들
문교수님이 박사과정할때만 해도, 배고프고 비실용적이라는 인식이 많았는데 이제 많이 변했음. 수학과 통계도 옛날엔 많이 이론위주였는데 많이 Application이 되었듯이 알고리즘도 같음.

- 카 네비게이션: 최단경로 알고리즘
- 스케줄링: TSP, 차량 라우팅, 작업공정, etc
- Human Genome Project: 매칭, 계통도, Functional analysis, etc
- 검색: DB, 웹페이지, etc
- 자원의 배치
- 반도체 설계: Partitioning, placement, routing, etc
- 등

TSP는 NP-hard 문제임. NP-hard, 그리고 NP-hard에 약간의 조건이 추가된 NP-complete에 대해서도 배움. NP-complete 군에 속하는 모든 문제는 바꾸어 풀 수 있음. NP-complete 군에 속하는 문제중 하나의 솔루션으로 다른 문제들도 같이 풀 수 있음. NP-complete 문제중 아직 단 한개도 현실적인 시간 안에 풀 수 있는 알고리즘이 발견되지 않았음. NP-complete 문제를 현실적인 시간 안에 풀지 못할거다라는 추측을 다들 강하게 하고있는데, 증명되진 않고있음.

차량 라우팅 문제는 두개의 NP-hard 문제가 중첩된 문제이다.

10년 전에 반도체 라인을 최적화하는 문제를 풀었었는데, 풀다보니 두개의 NP-hard 문제를 발견했음. 그 전까진 그 NP-hard 문제를 풀지 못하고 숙련공들의 감에 의존해서 풀고있었음.

Human Genome Project에서 알고리즘 엔지니어들이 기여를 많이 했음. String matching이 많이 쓰였다. DNA 시퀀스를 여러개로 잘게 잘라 각각의 작은 조각을 분석한 뒤, 전체를 추정하는 방식으로 분석했음.

계통도. 동물세포의 핵 DNA와 미토콘드리아 DNA를 분석해, 인류와 생물이 어떻게 분화했는지 추적할 수 있게 됨. 이 때 정확한 계통도는 알 수 없고 가장 확률이 높은 계통도를 사용하게되는데, 어려운 최적화 문제이고 NP-hard이다. 90년대에 에이즈 유전자 분석을 해서 한국에 에이즈가 어떻게 유입되었는지 분석한적 있었다. 분석결과 두명의 사람이 해외에서 독자적으로 에이즈에 감염되어왔다는 결론이 났는데, 이것도 알고리즘으로 푼것이다.

아마존에선 물류창고의 물건들을 자동으로 배치하고 관리하는데, 이런것도 최적화 문제이다.

80년대에 알고리즘 전공자들이 제대로된 대우를 받지 못하던 시절, VLSI CAD 분야에서 최적화 전문가들을 많이 고용했었다. 사람이 디자인하기엔 너무 크기때문에 대부분 알고리즘이 디자인한다. 삼성전자 한 회사에서 한해에 지불하는 VLSI CAD 이용료가 1000억이 넘음.

### 알고리즘을 왜 분석하는가
- 무결성을 확인하기위해
- 자원을 얼마나 효율적으로 쓰고있는지 파악하기위해

자원은 시간, 메모리, IO 대역폭 등 다양하다.

크기가 작다면? 알고리즘의 효율성이 그다지 중요하지 않다. 비효율적인 알고리즘도 별 상관 없음. 크기가 충분히 큰 문제여야 알고리즘의 효율성이 중요해진다. 비효율적인 알고리즘이 치명적인 결과를 냄

입력의 크기가 충분히 큰 경우에 대한 분석을 점근적 분석, Asymptotic analysis라고 한다. 이 수업에서 점근적인 분석이라는 말이 앞으로 아주 많이 나온다

### Asymptotic Analysis
입력의 크기가 충분히 큰 경우에 대한 분석. 이미 알고있는 점근적 개념의 예: lim n→∞ f(n)

알고리즘의 복잡도를 표현하기 위한 점근적 notation에선, 계수를 중요하게 사용하지 않는다. 최고차항을 제외한 차수가 낮은 항도 모두 무시한다. 왜냐면 n이 충분히 크면 최고차항만이 큰 차이를 내서.

Big O, Big Omega, Big Theta, little omega, little o 다섯가지 표기법이 있음.

Big O, Little O 두개가 관련이 있고, Big Omega, Little Omega 두개가 관련이 있음.

Big O, Big Omega는 대칭관계, Little O, Little Omega는 대칭관계. Big Theta는 Big O와 Big Omega의 교집합

### Asymptotic Notations
#### Big O
Big O: O(g(n)), 기껏해야 g(n)의 비율로 증가하는 함수.

ex: 3n^2 + 2n, 7n^2 - 100n, n^2 모두 O(n^2). 원래는 "n^2 ∈ O(n^2)" 이렇게 써야하는데, 관행적으로 "n^2 = O(n^2)" 이렇게 표기하기도 함. 같은 뜻임. "O(n^2) = n^2" 이렇게 쓰지는 않은다. 주의

Formal definition: O(g(n)) = {f(n) | ∃c > 0, n₀ ≥ 0 s.t. ∀n ≥ n₀, f(n) ≤ cg(n) }

"∃c > 0, n₀ ≥ 0 s.t. ∀n ≥ n₀"는 "충분히 큰 n에 대해서"를 수학적으로 쓴것임.

직관적 의미: f(n) = O(g(n)) ⇒ f가 g보다 빠르게 증가하지는 않는다 이런 뜻임

Big O notation을 쓰면 가능한한 타이트하게 써야한다. nlogn + 5nㅇ = O(nlogn) 인데 O(n^2)라고 써도 틀리진 않는다. 하지만 엄밀하지 않은 만큼 정보의 손실이 일어난다.

#### Big Omega
Formal definition: Ω(g(n)) = {f(n) | ∃c > 0, n₀ ≥ 0 s.t. ∀n ≥ n₀, f(n) ≥ cg(n) }

직관적 의미: f(n) = Ω(g(n)) ⇒ f는 g보다 느리게 증가하지는 않는다. O(g(n)) 과 대칭적

#### Big Theta
Formal definition: Θ(g(n)) = O(g(n)) ⋂ Ω(g(n))

직관적 의미: f(n) = Θ(g(n)) ⇒ f는 g와 같은 정도로 증가한다.

Big Theta로 이야기할 수 있으면 Big Theta로 이야기해야한다. 제일 Tight한 표기법이기 때문. 그러나 Big Theta로 이야기할 수 없는 경우가 많다.

#### Little O
Formal definition: o(g(n)) = {f(n) | lim n→∞ f(n)/g(n) = 0 }

직관적 의미: f(n) = o(g(n)) ⇒ f는 g보다 느리게 증가한다. (같은 경우가 빠졌음)

o(n^2) = { 9n + 4, nlogn, n^1.99, ... }

#### Little Omega
Formal definition: o(g(n)) = {f(n) | lim n→∞ f(n)/g(n) = inf }

직관적 의미: f(n) = ω(g(n)) ⇒ f는 g보다 빠르게 증가한다. (같은 경우가 빠졌음)

ω(n^2) = { 3n^3, n^2.01, 2^n, ... }

#### 예시
- Selection sort: Θ(n^2)
- Heap sort: O(nlogn)
- Quick sort: O(n^2), 평균 Θ(nlogn), At worst-case: Θ(n^2)

"Comparison sort는 최악의 경우 Ω(nlogn)이다"라는 정리가 있음.

### 시간 복잡도 분석의 종류
- Worst case: 알고리즘이 가장 느려지게 만드는 입력에 대한 분석
- Average case: 모든 경우에 대한 분석, 제일 분석하기 어려움
- Best case: 알고리즘이 가장 빨라지게 만드는 입력에 대한 분석, 그다지 유용하지 않음

보통은 worst case, average cast 분석을 함. Best case가 드물게 유용한데, insertion sort가 worst, average에 Θ(n^) 이지만, best case에는 Θ(n)임.

#### 저장/검색의 복잡도
1.  배열: O(n), 삽입, 삭제, 검색 중 적어도 하나는 Θ(n)
2.  Binary search tree: 최악의 경우 Θ(n), 평균 Θ(logn)
3.  Balanced binary search tree: 최악의 경우 Θ(logn)
4.  B-tree: 최악의 경우 Θ(logn)
5.  Hash table: 평균 Θ(1), 최악의 경우 Θ(n)

#### 크기가 n인 배열에서 원소 찾기
- Sequential search: Worst Θ(n), Average Θ(n), Best Θ(1)
- Binary search: Worst Θ(logn), Average Θ(logn), Best Θ(1)

&nbsp;

Week 2, Mon
========
## 3. Recurrence and Asymptotic Complexity Analysis, 점화식과 알고리즘의 복잡도

### 점화식, recurrence
어떤 함수를 자신보다 더 작은 변수에 대한 함수와의 관계로 표현한 것.

예시:

- f(n) = f(n-1) + 2
- f(n) = n * f(n-1)
- f(n) = f(n-1) + f(n - 2)
- f(n) = f(⌊n/2⌋) + n (보통 f(n/2) + n 라고 줄여 씀)
- ...

merge sort 수행시간을 점화식으로 표현하면 T(n) = 2*T(n/2) + 오버헤드.

### 점화식의 점근적 분석 방법
1.  반복대치, Iteration
    - 함수를 계속 더 작은 형태로 대치해나가기
2.  추정 후 증명, Guess & Verification
    - 결론을 추정하고, 수학적 귀납법으로 증명
3.  마스터 정리, Master Theoram
    - 형식에 맞는 점화식의 복잡도를 바로 알 수 있다

마스터 정리는 굉장히 유용함

### 소요시간 함수 T(n) 에 깔려있는 가정
1.  n은 양의 정수
2.  T는 단조증가
3.  If we need, Without loss of generality (WLOG), n = a^k 라고 가정할 수 있다.

3번이 왜 성립하냐면, a^k, a^k+1 들에 대해 증명이 성공한다면 a^k와 a^k+1 사이에 있는 숫자들에 대해서도 모두 성립하기때문.

### 반복대치, Iteration
#### Selection sort
T(n) = T(n-1) + n, T(1) 일때 반복대치로 T(n)을 구해보자.

```
T(n) = T(n-1) + n
     = T(n-2) + (n-1) + n
     = T(n-3) + (n-2) + (n-1) + n
     ...
     = T(1) + 2 + 3 + ... + n
     = 1 + 2 + ... + n
     = n(n+1)/2
     = Θ(n^2)
```

Selection sort같은 정렬 알고리즘들이 이런식으로 overhead가 Θ(n)인 점화식을 가져서, 죄다 Θ(n^2) 임.

#### Merge sort
1주차에서 한 머지소트 증명도 반복대치임

#### 네개로 나누기
T(n) = n + 3*T(n/4) 도 반복대치로 풀어서, `T(n) = 4n + (T(1) - 4)*n^k, k = log4/log3 = 0.7924...`로, T(n) = Θ(n) 인 것을 알 수 있다.

### 추정 후 증명, Guess & Verification
#### Merge sort
T(n) = 2T(n/2) + n 의 시간복잡도를 추정 후 증명으로 구해보자.

T(n) = O(nlogn) 이라고 가정해보자. 즉, T(n) ≤ cnlogn.

```
n보다 작은 모든 k에 대해 T(k) ≤ cklogk 가 성립한다고 할 때

T(n) = 2T(n/2) + n
     ≤ 2c(n/2)log(n/2) + n     (귀납적 대치, inductive substitution)
     = cnlogn + (1 - clog2)n
     ≤ cnlogn
```

수학적 귀납법에 의해 모든 충분히 큰 c와 n0에 대해 T(n) ≤ cnlogn 이 성립함. c와 n0는 적당히 크게 고르면 된다. 이 경우 c = 2, n0 = 4.

#### Merge sort 변형
T(n) = 2T(n/2 + 17) + n

답이 O(nlogn) 이라고 가정해보자.

```
n보다 작은 모든 k에 대해 T(k) ≤ cklogk 가 성립한다고 할 때,
n/2 + 17 < n 을 만족하는 n에 대해 (34 < n)
n/2 + 17 ≤ 3n/4 를 만족하는 n에 대해 (68 ≤ n)

T(n) = 2T(n/2 + 17) + n
     ≤ 2cT(n/2 + 17)log(n/2 + 17) + n
     ≤ 2cT(n/2 + 17)log(3n/4) + n
     = c*nlogn + (clog3/4 + 1)*n + 34clog(3n/4)
     ≤ cnlogn    for big enough n
```

역시 c와 n0 모두 충분히 크게 잡아주면 됨. c = 5

#### 직관과 배치되는 예
T(n) = 2T(n/2) + 1.

답이 O(n)라고 가정해보자. T(n) ≤ cn

```
T(n) = 2T(n/2) + 1
     ≤ 2c(n/2) + 1
     = cn + 1         ... 더이상 진행 불가
```

이런 경우 T(n) ≤ cn - 2 라고 가정해야함.

```
T(n) = 2T(n/2) + 1
     ≤ 2c(n/2) - 3
     ≤ cn - 2
```

수학적 귀납법으로 증명할때 보통 base case 증명을 해야한다. 그러나 알고리즘 증명할때엔 base case들은 다들 trivial하게 성립해서 생략하는경우가 많다. c를 자기 원하는대로 설정할 수 있기 때문이다.

### 마스터 정리
점화식이 아래와 같은 형태이면, 마스터 정리에 의해 바로 결과를 알 수 있다.

T(n) = aT(n/b) + f(n)

점화식이 위의 꼴일때 이를 계속 iterate하면

T(n) = Σ aⁱf(n/bⁱ) + n^(loga/logb)

앞 항을 Particular solution이라고 함. Overhead의 총합임.

뒤 항을 Homogeneous solution이라고 함. 크기가 1인 문제를 푸는 횟수. Boundary case를 푸는데에 드는 코스트. 아주 중요한 항. 분석의 잣대가 됨

Topmost overhead의 크기와, homogeneous term의 합을 비교해 시간복잡도를 알아낼 수 있음

#### 마스터 정리의 직관적 의미
T(n) = aT(n/b) + f(n) = Σ aⁱf(n/bⁱ) + n^(loga/logb)

위 식에서 Homogeneous term을 h(n)=n^(loga/logb) 과 같이 정의했을 때

1. h(n)이 더 무거우면 h(n)이 수행시간을 결정함
2. f(n)이 더 무거우면 f(n)이 수행시간을 결정함
3. h(n)과 f(n)이 같은 무게이면 logn*h(n) 이 수행시간이 됨

#### 마스터 정리
어떤 양의 상수 ε에 대해...

1. f(n)/h(n) = O(1/n^ε) 이면, T(n) = Θ(h(n))
2. f(n)/h(n) = Ω(n^ε) 이고, 충분히 큰 모든 n에 대해 a*f(n/b) < f(n) 이면, T(n) = Θ(f(n))
3. f(n)/h(n) = Θ(1) 이면, T(n) = Θ(h(n)logn)

"충분히 큰 모든 n에 대해 a*f(n/b) < f(n) 이면" 이 말은 iteration을 할수록 오버헤드가 작아져야한다는 뜻임. 대부분의 알고리즘들은 이 조건을 만족해줌.

사실 Master theorem의 full 버전은 여기에 하나 더해

- f(n)/h(n) = Θ((logn)^k) 이면, T(n) = Θ(h(n)*(logn)^(k+1))

#### 마스터 정리 예시
- T(n) = 2T(n/3) + c

  f(n) = c, h(n) = n^(log2/log3) = n ^ 0.63..

  1번 케이스에 해당, T(n) = Θ(n^(log2/log3)) = Θ(n ^ 0.63..)

- T(n) = 2T(n/4) + n

  f(n) = n, h(n) = n^0.5

  f(n)/h(n) = n^0.5 이고, 2*f(n/4) < f(n)

  2번 케이스에 해당, T(n) = Θ(n)

- T(n) = 2T(n/2) + n

  f(n) = n, h(n) = n

  f(n)/h(n) = 1

  3번 케이스에 해당, T(n) = Θ(nlogn)

슈트라센 알고리즘 시간복잡도 증명도 마스터 정리로 할 수 있음

&nbsp;

Week 2, Wed
========
Strassen Algorithm 시간복잡도 증명

#### Naive 행렬곱
행렬을 네개의 조각으로 분할하여 각각을 곱한 뒤 더할 수 있다. (PPT 참고) 하나의 큰 행렬곱을 여덟개의 작은 행렬곱으로 분할할 수 있음.

- r = ae + bf
- s = ag + bh
- t = ce + df
- u = cg + dh

이 경우 T(n) = 8T(n/2) + Θ(n^2) 이다. 마스터 정리에 의해 T(n) = Θ(n^(log8/log2)) = Θ(n^3) 임.

#### Strassen's algorithm
슈트라센 알고리즘은 위의 수식을 잘 조작해서, 하나의 큰 행렬곱을 여덟개가 아니라 일곱개로 줄인것이다.

- P1 = a(g-h)
- P2 = (a+b)h
- P3 = (c+d)e
- P4 = d(f - e)
- P5 = (a + d)(e + h)
- P6 = (b - d)(f + h)
- P7 = (a - c)(e + g)
- r = P5 + P4 - P2 + P6
- s = P1 + P2
- t = P3 + P4
- u = P5 + P1 - P3 - P7

T(n) = 7T(n/2) + Θ(n^2) 가 되어, 마스터 정리에 의해 T(n) = Θ(n^(log7/log2)) 이 됨.

슈트라센에서 이걸 발표했을때, 수학계 사람들이 경악했습니다. 제가 500번을 다시 태어나서 태어날때마다 계속 도전한다 해도, 내머리론 안될겁니다. 머리가 조금씩 좋아진다해도 안될거같아. 이런 사람을 앉혀놓고, 이런사람과 경쟁을 해야하는 환경이 주어진다면, 나는 교수 안합니다. 동해안에 커피숍 하나 차려가지고 책읽고 기타나 치면서 한평생 편안하게 사는걸 선택할겁니다. 내가 유학하던 시절에 박사과정에서 배웠는데, 그때 무슨 생각이 들었냐면, 내가 이걸 손으로 찾진 못하겠지만 이걸 찾는 문제공간 탐색알고리즘을 돌리면 어쩌면 가능할지도 모르겠다는 생각이 들었다. 근데 교수되고나니까 이걸 프로그램짜서 직접 하는게 힘들어지더라구요. 서울대 교수라는 자리가 로드가 굉장히 많은 자리입니다. 학교 밖에서도 굉장히 많은걸 요구하고, 시간적으로 힘듭니다. 똑똑한 학생들이 많으니 할수있는게 많기도 하겠지만.

대학원 유전알고리즘 수업에서 플젝으로 TSP나 지수귀문도를 자주 풉니다. TSP는 굉장히 유명한 NP-hard이고, 지수귀문도도 아마 NP-hard 군에 속할건데, 조선시대에 영의정을 지낸 최석정이 고안했다고 알려진 문제입니다. TSP보다 더 어려운 문제임. 이걸 유전알고리즘으로 푸는겁니다. 근데 이것과 아울러서 또 학생들한테 주는게, 슈트라센 알고리즘이라는게 있다, 슈트라센처럼 7개 곱셈으로 행렬곱을 최적화할 수 있는지 유전알고리즘으로 도전해보고싶은사람은 도전해봐라 같이 줍니다. 이거 하는사람은 TSP나 지수귀문도 안해도 된다. 한학기에 한둘씩 이걸 도전하는데, 모든 학생들이 예외 없이 처참한 결과를 냈습니다. 슈트라센 도전했던 사람들이 모두 C를 받았음. 도전정신이 굉장히 강한 학생들이었기때문에 높은 점수를 주고싶은데, 다들 프로젝트를 시작했다고 볼 수 없는 수준에서 그쳐서, 높은 점수를 줄수가 없었음.

그러다가, 2000년대 중반에 학부생 한명이 대학원 수업에 와서 유전알고리즘으로 이걸 도전하겠다고 했다. 본인이 유학을 가겠다고 하는데, 이걸 도전하겠다고 해서 걱정했다. 유학가는데엔 학부 학점이 중요하니. 이거 했다가 C 받으면 어쩌지? 이 학생은 결국 시작했고, 다른 학생들과 다르게 나를 찾아오기 시작합니다. 이거 모든경우의수를 다 하는데엔 영겁의 시간이 걸리고, 선형대수 Rank 테크닉같은걸 동원해서 공간을 좀 줄이면 다 하는데에 데스크탑 PC로 6700만년정도 걸리겠더라구요. 근데 이학생이 5월 중순경에 연락도 없이 뛰어와서 찾았다는겁니다. 믿지 않았어요. 근데 오류가 없는거에요. 그렇게 Strassen's Algorithm과 다른 방법으로 해를 하나 더 찾은겁니다. 그래서 고무가 되어서 더 열심히 찾고, a b c 계수로 {-1, 0, 1} 뿐만 아니라 {-1, -0.5, 0, 0.5, 1} 도 허용하면서 문제공간을 늘려서 더 찾고 그랬는데, 적어도 슈트라센 알고리즘과 다른 608가지 방법이 더 있다는것을 찾아냈습니다. 슈트라센은 608개중에 하나, 위노그라드는 608개중에 다른 하나를 찾은거였습니다. 그래서 그 결과를 학부생이 IEEE Transactions on Evolutionary Computation에 게재했습니다. 지금은 IEEE Transactions on Evolutionary Computation가 IEEE 저널중 Impact Factor가 가장 큰 저널입니다. 그 학생은 미국유학을 가서도 계속 논문을 집필했고, 결국 publish 됩니다.

[Oh, Seunghyun, and Byung-Ro Moon. "Automatic reproduction of a genius algorithm: Strassen's algorithm revisited by genetic search." IEEE Transactions on Evolutionary Computation 14.2 (2009): 246-251.](http://rosaec.snu.ac.kr/publish/2010/T2/OhMo-TEC-accepted.pdf)

컴퓨터의 공간탐색능력은 대단하다. 알파고도 문제공간을 랜덤으로 근사적으로 탐색하는 문제공간 탐색엔진이라고 볼 수 있음. 딥러닝은 탐색하지 않아도 되는 문제공간을 줄이는데에 사용되는것이고. 알파고 나왔을때 많은 바둑 기사들이 일자리를 잃게된다고 탄식했지만 전 신문칼럼에 그렇지 않을거라고 글을 썼죠. 인간과 AI가 공존할거고, 인간의 착상 방법이 알파고로 인해 더 넓어질거다. 그리고 실제로 그렇게 되었습니다. 지금 프로기사들은 인공지능 바둑 패키지 없이는 공부를 할 수 없게 되었습니다.

&nbsp;

## 4. 정렬
정렬 자체도 중요하지만, 정렬문제를 푸는 다양한 관점, 그 과정에서 생각하는 방법을 배우는게 더 중요합니다. 재귀적 사고를 배우는데에 아주 좋습니다.

대부분 O(n^2)과 Ω(nlogn) 사이이다. 그러나 Input이 특수한 성질을 만족하면 Θ(n) 정렬도 가능하다. 예: 입력이 -O(n)과 O(n) 사이일경우 Θ(n)인 counting sort가 가능.

n^2짜리 원시적인 정렬 알고리즘을 먼저 배우고, 그 다음은 nlogn짜리 advanced 정렬 알고리즘, 그다음엔 특수한 성질을 만족하는 경우의 n짜리 정렬 알고리즘에 대해 배운다.

그리고 Comparison sort는 항상 Ω(nlogn)에 속하는데, 이 정렬의 하한성에 대해서도 배운다.

### 원시적인 Sorting 알고리즘들의 재조명
알고리즘에 보는 시각에 크게 두개가 있다

- flow 중심
- 관계 중심

원시적 정렬 알고리즘들은 대부분 flow 중심이다. 이걸 관계 중심으로 다시 생각해보자. 생각하는 방법에 대한 좋은 연습자료다.

#### Selection sort
각 루프마다..

- 최대 원소를 찾는다
- 최대 원소와 맨 오른쪽 원소를 교환
- 맨 오른쪽 원소를 제외한다

이 과정을 하나의 원소만 남을때까지 반복.

T(n) = n(n-1)/2 = Θ(n^2). Worst, Average, Best 무관하게 Θ(n^2) 임

보통 Selection sort를 설명할때 2중 loop로 표현하지만, 사실 재귀적인 성격이 있음. 재귀를 한번 돌때마다 배열의 길이가 1 줄어드는 재귀함수임.

```
fn selection_sort(list: Vec<T>) {
     // Base case
     if list.len() == 1 { return list }

     // Overhead
     list에서 가장 큰 수 list[k] 를 찾는다
     list[k]와 list[-1] 스왑

     // 재귀
     return selection_sort(list[..-1])
}
```

수학적 귀납법으로 증명할 수 있다.

증명: 입력으로 정렬되지 않은 배열 A0가 주어졌다. 배열 A0의 길이는 n이다. selection sort pass를 i번 돈것을 Ai 라고 하자.

Corollary 1: ∀i ∈ {1, 2, ... , n}, A(i-1)[-(i-1)..] = Ai[-(i-1)..] 이다. A(i-1)을 Ai로 바꾸는게 i번째 selection sort pass인데, i번째 selection sort에선 A(i-1)[-(i-1)..] 범위는 조작하지 않기 때문이다.

Corollary 2: ∀i ∈ {1, 2, ... , n}, Ai[-i] 은 Ai[..-i] 안에있는 모든 원소들보다 크거나 같다. i번째 selection sort pass가 하는 일 중 하나가, A(i-1)[..-(i-1)] 에서 제일 큰 원소를 Ai[-i]에 대입시키는 것이기 때문이다.

명제 p(i) = "Ai[-i..]는 단조증가한다" 가 있을때, p(i)가 ∀i ∈ {1, 2, ... , n} 에 대해 성립함을 수학적 귀납법으로 증명해보자.

1.  p(1)는 참이다.

    A1[-1]은 하나짜리 배열이므로 단조증가한다.

2.  ∀i ∈ {1, 2, ... , n-1}, p(i)가 참이면, p(i+1)도 참이다.

    Corollary 2에 의해, Ai[-i]는 Ai[..-i] 에 있는 모든 원소들보다 크거나 같다.

    i번째 Selection sort의 원리 상, A(i+1)[-(i+1)] 은 Ai[..i] 에 있는 원소다.

    ⇒ Ai[-i]는 A(i+1)[-(i+1)]보다 크거나 같다.

    Corollary 1에 의해, Ai[-i] == A(i+1)[-i].

    ⇒ A(i+1)[-i]는 A(i+1)[-(i+1)] 보다 크거나 같다.

    ⇒ A(i+1)[-(i+1)..] 는 단조증가한다

수학적 귀납법에 의해 ∀i ∈ {1, 2, ... , n}, Ai[-i..] 는 단조증가한다.

i에 n을 대입하면, An[-n..] = An 은 단조증가하므로 An은 정렬되어있다. Selection sort pass를 n회 반복하면 배열이 정렬됨을 증명하였다.

#### Bubble Sort
T(n) = n(n-1)/2 = Θ(n^2)

형태는 다르지만 Selection Sort와 거의 비슷함. Pass 한번 돌 때마다 풀어야하는 배열의 크기가 1씩 줄어듬. 얘도 재귀적으로 표현하면

```
fn bubble_sort(list: Vec<T>) {
     // Base case
     if list.len() == 1 { return list }

     // Overhead
     for i in 0..list.len() - 1 {
          if A[i] > A[i+1] {
               A[i]와 A[i+1] 스왑
          }
     }

     // 재귀
     return bubble_sort(list[..-1])
}
```

얘도 선택정렬이랑 비슷하게 증명하면 된다.

#### Insertion Sort
Pass 한번 돌 때마다, 정렬된 배열의 크기가 1씩 늘고, 정렬되지 않은 배열의 크기가 1씩 준다.

- Worst case: 1 + 2 + ... + (n-1) = n(n-1)/2 = Θ(n^2)
- Average case: 0.5 * (1 + 2 + ... + (n-1)) = n(n-1)/4 = Θ(n^2)
- Best case: 1 + 1 + ... + 1 = n = Θ(n)

Insertion Sort가 동작한다는걸 고등학교에서 배운 수학적 귀납법으로 증명할 수 있음

- 배열 A[0..0]만 보면: 원소 하나니까 정렬되어있음
- 배열 A[0..k]가 정렬되어있을 때, insertion sort의 루프 1 pass를 돌리면 A[0..k+1] 이 정렬됨

수학적 귀납법에 의해 insertion sort를 계속 돌리면 배열 전체가 정렬된다.

두 버전의 재귀가 있음

```
fn insertion_sort(list: Vec<T>, sorted: usize) {
     // list를 정렬하고싶음
     // list[0..sorted] 는 정렬되어있음

     // Base case
     if sorted >= list.len() - 1 { return list }

     // Overhead
     list[sorted..] 에서 하나를 뽑아 list[0..sorted] 의 적당한 위치에 삽입함

     // 재귀
     return insertion_sort(list, sorted + 1);
}
```

```
fn insertion_sort(list: Vec<T>) {
     // Base case
     if list.len() == 1 { return list }

     // 재귀
     list[..-1] = insertion_sort(list[..-1])

     // Overhead
     list[-1] 을 list[..-1] 의 적당한 위치에 삽입함
}
```
